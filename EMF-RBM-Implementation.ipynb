{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test implementing basic EMF iterations\n",
    "\n",
    "TODO:  check second iteration--why is it 'off' -- why don't h values change ?\n",
    "\n",
    "- work through by hand...maybe print out  \n",
    "- because we are sampling ?  \n",
    "\n",
    "implement:  get free energy, entropy, and energy distributions\n",
    "\n",
    "test on mnist:  can we measure entropy, energy, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:01.011550",
     "start_time": "2016-09-30T15:32:58.909290"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:01.502330",
     "start_time": "2016-09-30T15:33:01.013338"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import expit    \n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:01.508323",
     "start_time": "2016-09-30T15:33:01.504270"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sig_means(x, b, W):\n",
    "    a = safe_sparse_dot(x, W.T) + b\n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use julia data set\n",
    "\n",
    "I don't know how to reproduce their normalization yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:05.929630",
     "start_time": "2016-09-30T15:33:01.511057"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'HDF5.name___X', u'HDF5.name___y'])\n",
      "(60000, 784) (60000,)\n",
      "norm of X  2117.63422548\n"
     ]
    }
   ],
   "source": [
    "hf =  h5py.File('mnist.h5','r')\n",
    "print('List of arrays in this file: \\n', hf.keys())\n",
    "X = np.array(hf.get('HDF5.name___X'))\n",
    "Y = np.array(hf.get('HDF5.name___y'))\n",
    "print X.shape, Y.shape\n",
    "hf.close()\n",
    "print \"norm of X \",np.linalg.norm(X,ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-14T22:24:29.473192",
     "start_time": "2016-09-14T22:24:29.470744"
    }
   },
   "source": [
    "### plot the new mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:05.944199",
     "start_time": "2016-09-30T15:33:05.932829"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 array of pixel data.\n",
    "    Reshape array, and rotate for display\n",
    "    \"\"\"\n",
    "    square_image = np.rot90(np.reshape(image, [28,28]))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(square_image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:06.943782",
     "start_time": "2016-09-30T15:33:05.947028"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC61JREFUeJzt3U+sZnV9x/H3x5IulAQmRGYahg41TRo3ZGKjG7p4TBMl\n3QxxgZYNmMawKJa0GymbexddtC5I6MKNoBkaSaskCphYscGEsFAIdMqgI5o0Q4syt2rAMDvT+XZx\nD8Nl+tz7PNzn773f9yt5wnnOc88933vmfJ7f75zfOYdUFZJ6ed+qC5C0fAZfasjgSw0ZfKkhgy81\nZPClhpYW/CS3JvlJkp8m+cKy1jutJOeT/EeSf0/y3BrU83CSrSQv7Zh3JMlTSV5J8t0k16xZfRtJ\nXkvy4vC6dYX1HU/ydJIfJTmb5K+G+WuxDcfU9/lh/lK2YZYxjp/kfcBPgT8FfgE8D3ymqn6y8JVP\nKcl/An9cVW+suhaAJH8CXAQeqaqbh3n/APy6qr44fHkeqar71qi+DeCtqnpgFTXtlOQYcKyqziS5\nGngBOAV8ljXYhnvU92mWsA2X1eJ/DPhZVb1aVb8F/pntP3KdhDU69KmqZ4Erv4ROAaeH6dPAbUst\naodd6oPt7bhyVXWhqs4M0xeBc8Bx1mQb7lLfDcPHC9+Gy9rRbwD+e8f713jnj1wXBXwvyfNJPrfq\nYnZxfVVtwfaOA1y/4nrGuSfJmSQPrfJQZKckNwEngR8AR9dtG+6o74fDrIVvw7Vp4dbALVX1EeDP\ngL8curLrbt2ut/4S8KGqOglcANahy3818Bhw79CyXrnNVroNx9S3lG24rOD/HPj9He+PD/PWRlW9\nPvz3l8A32T48WTdbSY7C5WPE/1lxPe9SVb+sd04afRn46CrrSXIV26H6p6p6fJi9NttwXH3L2obL\nCv7zwB8mOZHkd4HPAE8sad0TJXn/8M1Lkg8AnwBeXm1VwPax3s7jvSeAu4bpO4HHr1xgyd5V3xCk\nt32K1W/DrwA/rqoHd8xbp234/+pb1jZcyll92B7OAx5k+8vm4ar6+6WseApJ/oDtVr6Aq4Cvrbq+\nJI8CI+A6YAvYAL4FfAO4EXgVuL2q3lyj+j7O9rHqJeA8cPfbx9MrqO8W4BngLNv/rgXcDzwHfJ0V\nb8M96ruDJWzDpQVf0vrw5J7UkMGXGjL4UkMGX2popuCv+403ksbb91n9aW+8SeKwgbQiVTX2uv9Z\nWvypb7ypqsuvjY2Nd71ft5f1Hd761rm2RdS3l1mCfxBuvJE0hif3pIaummHZqW+82dzcvDx97bXX\nzrDKxRuNRqsuYU/Wt3/rXBsst75ZTu79DvAK2yf3Xmf7Gug/r6pzV/xc7XcdkvYvCbXLyb19t/hV\n9b9J7gGe4p0bb85NWEzSGlj4TTq2+NJq7NXie3JPasjgSw0ZfKkhgy81ZPClhgy+1JDBlxoy+FJD\nBl9qyOBLDRl8qSGDLzVk8KWGDL7UkMGXGprl0VsHQjL2duQ2fBaCxrHFlxoy+FJDBl9qyOBLDRl8\nqSGDLzVk8KWGDv04fnezXsfgdQCHky2+1JDBlxoy+FJDBl9qyOBLDRl8qSGDLzU00zh+kvPAb4BL\nwG+r6mPzKGqeZh2H7n4/v9cBHE6zXsBzCRhV1RvzKEbScsza1c8cfoekJZs1tAV8L8nzST43j4Ik\nLd6sXf1bqur1JB9k+wvgXFU9e+UPbW5uXp4ejUaMRqMZVytpFpnXyZckG8BbVfXAFfPrIJ/g6X5y\nb1YH+d/+oEtCVY3dgffd1U/y/iRXD9MfAD4BvLzf3ydpeWbp6h8Fvpmkht/ztap6aj5lSVqkuXX1\nd13BAe/qz8pDhdl03ndmtZCuvqSDy+BLDRl8qSGDLzVk8KWGDL7UkMGXGvK5+gvm8wBmM+nvd5x/\nf2zxpYYMvtSQwZcaMvhSQwZfasjgSw0ZfKkhx/HXnNcB7M1x/v2xxZcaMvhSQwZfasjgSw0ZfKkh\ngy81ZPClhhzHP+S6XwfgOP94tvhSQwZfasjgSw0ZfKkhgy81ZPClhgy+1NDE4Cd5OMlWkpd2zDuS\n5KkkryT5bpJrFlumVqWqFvrSakzT4n8V+OQV8+4D/q2q/gh4GvjbeRcmaXEmBr+qngXeuGL2KeD0\nMH0auG3OdUlaoP0e419fVVsAVXUBuH5+JUlatHldq7/nwdrm5ubl6dFoxGg0mtNqJe1HpjnBkuQE\n8GRV3Ty8PweMqmoryTHg+1X14V2WLU/iaDervgnoMO+bSaiqsRt42q5+htfbngDuGqbvBB7fd3WS\nlm5ii5/kUWAEXAdsARvAt4BvADcCrwK3V9Wbuyxvi69d2eIvzl4t/lRd/RlXbvAPsVUHd5LO+948\nuvqSDhGDLzVk8KWGDL7UkMGXGjL4UkMGX2rI5+prT+s+Tq/9scWXGjL4UkMGX2rI4EsNGXypIYMv\nNWTwpYYcx2/uoI/Td77ffha2+FJDBl9qyOBLDRl8qSGDLzVk8KWGDL7UkOP4h5zj9BrHFl9qyOBL\nDRl8qSGDLzVk8KWGDL7UkMGXGpoY/CQPJ9lK8tKOeRtJXkvy4vC6dbFlajdJ9nwtWlUt9KXFmKbF\n/yrwyTHzH6iqjwyvf51zXZIWaGLwq+pZ4I0xHx3sS8KkxmY5xr8nyZkkDyW5Zm4VSVq4THMcleQE\n8GRV3Ty8/yDwq6qqJH8H/F5V/cUuy9bGxsbl96PRiNFoNI/axeqvxfc4fH0loarG7iD7Cv60nw2f\nlzvH4hh87Wav4E/b1Q87jumTHNvx2aeAl/dfnqRlm3hbbpJHgRFwXZL/AjaAjyc5CVwCzgN3L7BG\nSXM2VVd/phXY1V8ou/razTy6+pIOEYMvNWTwpYYMvtSQwZcaMvhSQwZfasjn6q/YqsfhJ3Gc/nCy\nxZcaMvhSQwZfasjgSw0ZfKkhgy81ZPClhhzHX7B1H6efZFL9k8b5Z/37vY5gMWzxpYYMvtSQwZca\nMvhSQwZfasjgSw0ZfKkhx/FndNDH6WfV/e8/qGzxpYYMvtSQwZcaMvhSQwZfasjgSw0ZfKmhieP4\nSY4DjwBHgUvAl6vqH5McAf4FOAGcB26vqt8ssFY1tO7XCRzU5wVkigcpHAOOVdWZJFcDLwCngM8C\nv66qLyb5AnCkqu4bs3wd1I0zjXXfMbVY67xvJ6Gqxu6gE7v6VXWhqs4M0xeBc8BxtsN/evix08Bt\n8ylX0qK9p2P8JDcBJ4EfAEeragu2vxyA6+ddnKTFmPpa/aGb/xhwb1VdTHJlH2fXPs/m5ubl6dFo\nxGg0em9VSpqricf4AEmuAr4NfKeqHhzmnQNGVbU1nAf4flV9eMyyHuPr0FrnfXumY/zBV4Afvx36\nwRPAXcP0ncDj+65Q0lJNc1b/FuAZ4Czb3fkC7geeA74O3Ai8yvZw3ptjlrfF16G1zvv2Xi3+VF39\nGVd+qIM/iV8Mva1y359HV1/SIWLwpYYMvtSQwZcaMvhSQwZfasjgSw35XP0FW/U1DF5HoHFs8aWG\nDL7UkMGXGjL4UkMGX2rI4EsNGXypIcfxD7lVX0eg9WSLLzVk8KWGDL7UkMGXGjL4UkMGX2rI4EsN\nGXypIYMvNWTwpYYMvtSQwZcaMvhSQwZfamhi8JMcT/J0kh8lOZvk88P8jSSvJXlxeN26+HIlzUMm\n3a+d5BhwrKrOJLkaeAE4BXwaeKuqHpiwfHlPuLR8Saiqsf9jhYkP4qiqC8CFYfpiknPADW//7rlV\nKWlp3tMxfpKbgJPAD4dZ9yQ5k+ShJNfMuTZJCzJ18Idu/mPAvVV1EfgS8KGqOsl2j2DPLr+k9THx\nGB8gyVXAt4HvVNWDYz4/ATxZVTeP+aw2NjYuvx+NRoxGo1lqljSFvY7xpw3+I8Cvqupvdsw7Nhz/\nk+SvgY9W1R1jlvXknrQCMwU/yS3AM8BZoIbX/cAdbB/vXwLOA3dX1daY5Q2+tAIzt/gzrtzgSyuw\nV/C9ck9qyOBLDRl8qSGDLzVk8KWGDL7UkMGXGjL4UkMGX2rI4EsNGXypIYMvNWTwpYYMvtSQwZca\nMvhSQxMfrz0PiU/hltbJwp/AI2n92NWXGjL4UkMGX2rI4EsNGXypof8D2lTiJUzX/xYAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1126bdb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+FJDBl9q6P8ANPZ2aSBSuCUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d570350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118a68f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC2hJREFUeJzt3U+MXeV5x/HvL7W6SCwZhGJcYQqNKlXZICtVsnEXN6qU\noG5AWZCUDURVxKIkqN2EsplZdNFmgcQmm0IipwqqEqQEiJTiVERCLBIQ1MUQh1SqTEuLp0kEFd5F\n9dPFHJvBvTP3Mvfv+Pl+pCufe+6cOc+ce3/3fd/zz6kqJPXyoVUXIGn5DL7UkMGXGjL4UkMGX2rI\n4EsNLS34SW5P8vMkv0jy1WWtd1pJzif5lyT/nOSFNajnsSRbSV7ZMe/6JKeTvJ7kmSRH1qy+jSRv\nJnl5eNy+wvqOJ3k2yWtJzib5yjB/LbbhmPq+PMxfyjbMMo7jJ/kQ8Avgj4H/Al4EvlBVP1/4yqeU\n5N+AP6yqt1ddC0CSPwIuAt+qqtuGeX8L/LqqvjZ8eV5fVQ+uUX0bwLtV9fAqatopyTHgWFWdSXIY\neAm4A/gia7AN96jv8yxhGy6rxf8U8K9V9UZV/Qb4B7b/yHUS1mjoU1XPA1d/Cd0BnBqmTwF3LrWo\nHXapD7a348pV1YWqOjNMXwTOAcdZk224S303DS8vfBsu64N+E/AfO56/yXt/5Loo4EdJXkzypVUX\ns4ujVbUF2x8c4OiK6xnn/iRnkjy6yqHITkluBU4APwFuXLdtuKO+nw6zFr4N16aFWwMnq+oTwJ8A\nfz50Zdfdup1v/XXgY1V1ArgArEOX/zDwBPDA0LJevc1Wug3H1LeUbbis4P8n8Ls7nh8f5q2Nqnpr\n+PeXwPfYHp6sm60kN8KVMeJ/r7ie96mqX9Z7O43+DvjkKutJcojtUP19VT05zF6bbTiuvmVtw2UF\n/0Xg95PckuS3gS8ATy1p3RMl+fDwzUuSjwCfAV5dbVXA9lhv53jvKeDeYfoe4MmrF1iy99U3BOmy\nz7H6bfgN4GdV9ciOeeu0Df9ffcvahkvZqw/bh/OAR9j+snmsqv5mKSueQpLfY7uVL+AQ8O1V15fk\ncWAE3ABsARvA94HvAjcDbwB3VdU7a1Tfp9keq14CzgP3XR5Pr6C+k8BzwFm239cCHgJeAL7Dirfh\nHvXdzRK24dKCL2l9uHNPasjgSw0ZfKkhgy81NFPw1/3CG0nj7Xuv/rQX3iTxsIG0IlU19rz/WVr8\nqS+8qaorj42Njfc9X7eH9V279a1zbYuoby+zBP8gXHgjaQx37kkNHZph2akvvNnc3Lwyfd11182w\nysUbjUarLmFP1rd/61wbLLe+WXbu/RbwOts7995i+xzoP62qc1f9XO13HZL2Lwm1y869fbf4VfW/\nSe4HTvPehTfnJiwmaQ0s/CIdW3xpNfZq8d25JzVk8KWGDL7UkMGXGjL4UkMGX2rI4EsNGXypIYMv\nNWTwpYYMvtSQwZcaMvhSQwZfasjgSw3NcustTSEZezn00ngvBI1jiy81ZPClhgy+1JDBlxoy+FJD\nBl9qyOBLDRl8qSGDLzVk8KWGDL7UkMGXGjL4UkMGX2rI4EsNzXQ9fpLzwP8Al4DfVNWn5lGUpMWa\n9UYcl4BRVb09j2IkLcesXf3M4XdIWrJZQ1vAj5K8mORL8yhI0uLN2tU/WVVvJfko218A56rq+at/\naHNz88r0aDRiNBrNuFpJs8i8bsaYZAN4t6oevmp+db7hozfb1KokoarGfgD33dVP8uEkh4fpjwCf\nAV7d7++TtDyzdPVvBL6XpIbf8+2qOj2fsiQt0ty6+ruuwK7+qkuYSef37qBbSFdf0sFl8KWGDL7U\nkMGXGjL4UkMGX2rI4EsNzXquviaYdBz8oB/n18Fkiy81ZPClhgy+1JDBlxoy+FJDBl9qyOBLDXkc\nf8U8zq9VsMWXGjL4UkMGX2rI4EsNGXypIYMvNWTwpYY8jq89TTqPwPvuH0y2+FJDBl9qyOBLDRl8\nqSGDLzVk8KWGDL7U0MTgJ3ksyVaSV3bMuz7J6SSvJ3kmyZHFlilpnqZp8b8JfPaqeQ8C/1RVfwA8\nC/zVvAuTtDgTg19VzwNvXzX7DuDUMH0KuHPOdUlaoP2O8Y9W1RZAVV0Ajs6vJEmLNq9z9fc8YXtz\nc/PK9Gg0YjQazWm1kvYj01xkkeQW4Omqum14fg4YVdVWkmPAj6vq47ssW17IsX/rfrNN39v1lYSq\nGvsBmrarn+Fx2VPAvcP0PcCT+65O0tJNbPGTPA6MgBuALWAD+D7wXeBm4A3grqp6Z5flbfFnYIuv\n/dqrxZ+qqz/jyg3+AvnFoN3Mo6sv6Rpi8KWGDL7UkMGXGjL4UkMGX2rI4EsNGXypIYMvNWTwpYYM\nvtSQwZcaMvhSQwZfasjgSw3N6557WpEpbqSypEr2t36v118NW3ypIYMvNWTwpYYMvtSQwZcaMvhS\nQwZfasjj+Nc4j/NrHFt8qSGDLzVk8KWGDL7UkMGXGjL4UkMGX2poYvCTPJZkK8krO+ZtJHkzycvD\n4/bFlilpnqZp8b8JfHbM/Ier6hPD4x/nXJekBZoY/Kp6Hnh7zEurPeVL0r7NMsa/P8mZJI8mOTK3\niiQtXKY5FzrJLcDTVXXb8PyjwK+qqpL8NfA7VfVnuyxbGxsbV56PRiNGo9E8atccrPpc/Uk8V3//\nklBVY9/gfQV/2teG18s3b30Z/GvXXsGftqsfdozpkxzb8drngFf3X56kZZt4WW6Sx4ERcEOSfwc2\ngE8nOQFcAs4D9y2wRklzNlVXf6YV2NU/0BwKHFzz6OpLuoYYfKkhgy81ZPClhgy+1JDBlxoy+FJD\n3ldfe1r3+/Jrf2zxpYYMvtSQwZcaMvhSQwZfasjgSw0ZfKkhj+PrQJt0HoHX649niy81ZPClhgy+\n1JDBlxoy+FJDBl9qyOBLDXkcXzNZ9+v1Z13/tXoegC2+1JDBlxoy+FJDBl9qyOBLDRl8qSGDLzU0\nMfhJjid5NslrSc4m+cow//okp5O8nuSZJEcWX64Omqra86HVyBQnYBwDjlXVmSSHgZeAO4AvAr+u\nqq8l+SpwfVU9OGb58g3WblZ9gs8kB/mzm4SqGruBJ7b4VXWhqs4M0xeBc8BxtsN/avixU8Cd8ylX\n0qJ9oDF+kluBE8BPgBuragu2vxyAo/MuTtJiTH2u/tDNfwJ4oKouJrm6D7Rrn2hzc/PK9Gg0YjQa\nfbAqJc3VxDE+QJJDwA+AH1bVI8O8c8CoqraG/QA/rqqPj1nWMb525Rh/cWYa4w++AfzscugHTwH3\nDtP3AE/uu0JJSzXNXv2TwHPAWba78wU8BLwAfAe4GXgDuKuq3hmzvC2+dmWLvzh7tfhTdfVnXLnB\n164M/uLMo6sv6Rpi8KWGDL7UkMGXGjL4UkMGX2rI4EsNeV99rdS635f/WmWLLzVk8KWGDL7UkMGX\nGjL4UkMGX2rI4EsNeRxfa+0gXw+/zmzxpYYMvtSQwZcaMvhSQwZfasjgSw0ZfKkhgy81ZPClhgy+\n1JDBlxoy+FJDBl9qyOBLDU0MfpLjSZ5N8lqSs0m+PMzfSPJmkpeHx+2LL1fSPGSK+5ofA45V1Zkk\nh4GXgDuAzwPvVtXDE5Yvr6mWli8JVTX2PyaYeCOOqroAXBimLyY5B9x0+XfPrUpJS/OBxvhJbgVO\nAD8dZt2f5EySR5McmXNtkhZk6uAP3fwngAeq6iLwdeBjVXWC7R7Bnl1+Setj4hgfIMkh4AfAD6vq\nkTGv3wI8XVW3jXmtNjY2rjwfjUaMRqNZapY0hb3G+NMG/1vAr6rqL3fMOzaM/0nyF8Anq+ruMcu6\nc09agZmCn+Qk8BxwFqjh8RBwN9vj/UvAeeC+qtoas7zBl1Zg5hZ/xpUbfGkF9gq+Z+5JDRl8qSGD\nLzVk8KWGDL7UkMGXGjL4UkMGX2rI4EsNGXypIYMvNWTwpYYMvtSQwZcaMvhSQwZfamji7bXnIfEu\n3NI6WfgdeCStH7v6UkMGX2rI4EsNGXypIYMvNfR/0vFX7SoYyJEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ba6f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDRl8qSGDLzX0f9KRhPnmW81QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118cfa2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    show(X[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-14T22:30:01.920393",
     "start_time": "2016-09-14T22:30:01.916657"
    }
   },
   "source": [
    "Notice that the images are binary now, not greyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-08-30T14:00:33.306668",
     "start_time": "2016-08-30T14:00:33.304102"
    }
   },
   "source": [
    "### Implement RBM and Test\n",
    "\n",
    "Eventually need to convert to sklearn code\n",
    "\n",
    "Using as much as existing RBM code as I can now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:06.953160",
     "start_time": "2016-09-30T15:33:06.945813"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.utils.extmath import log_logistic\n",
    "from sklearn.utils.fixes import expit             # logistic function\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:06.965670",
     "start_time": "2016-09-30T15:33:06.955799"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_v_bias(X):\n",
    "    # If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "    # initialize the visibile biases according to the empirical expected                                                                                \n",
    "    # feature values of the training data.                                                                                                              \n",
    "    #                                                                                                                                                   \n",
    "    # TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "    #       the case of binary RBMs.\n",
    "    eps = 1e-8\n",
    "\n",
    "    probVis = np.mean(X,axis=0)             # Mean across  samples \n",
    "    print np.linalg.norm(X[0,:], ord=2)\n",
    "    print \"mean X\",np.linalg.norm(probVis, ord=2)\n",
    "\n",
    "    probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "    print \"norm probVis\",np.linalg.norm(probVis, ord=2)\n",
    "\n",
    "    #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "    v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion  \n",
    "    return v_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:07.010184",
     "start_time": "2016-09-30T15:33:06.967956"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EMF_RBM():\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100, sigma=0.001, neq_steps = 3,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1'):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "            \n",
    "        self.sigma = sigma\n",
    "        self.neq_steps = neq_steps\n",
    "        \n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.rng = check_random_state(random_state)\n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.rng.normal(\n",
    "                0,\n",
    "                sigma,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "        \n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "\n",
    "\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        #self.v_bias = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        \n",
    "        # learning rate / mini_batch\n",
    "        self.lr = 0.0\n",
    "        \n",
    "        print \"W emf_init norm \", np.linalg.norm(self.W, ord=2)\n",
    "\n",
    "        # init vbias\n",
    "        print X[0:50000,:].shape\n",
    "        self.v_bias = init_v_bias(X[0:50000,:])\n",
    "        print \"v bias emf_init shape \",self.v_bias.shape\n",
    "\n",
    "        print \"v bias emf_init norm \", np.linalg.norm(self.v_bias, ord=2)\n",
    "        print \"h bias emf_init norm \", np.linalg.norm(self.h_bias, ord=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:07.078326",
     "start_time": "2016-09-30T15:33:07.012475"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W emf_init norm  4.41857505506e-19\n",
      "(50000, 784)\n",
      "12.8840987267\n",
      "mean X 8.43148429713\n",
      "norm probVis 8.43148429713\n",
      "v bias emf_init shape  (784,)\n",
      "v bias emf_init norm  195.129720728\n",
      "h bias emf_init norm  0.0\n"
     ]
    }
   ],
   "source": [
    "rbm = EMF_RBM(momentum=0.5, decay=0.01, learning_rate=0.005, n_iter=20, sigma=1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:07.085020",
     "start_time": "2016-09-30T15:33:07.080808"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_layer(rbm, layer):\n",
    "    return (rbm.rng.random_sample(size=l.shape) < layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:33:07.091655",
     "start_time": "2016-09-30T15:33:07.087603"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sample_hiddens(rbm, v):\n",
    "    return sample_layer(rbm, _mean_hiddens(rbm, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T22:00:13.175729",
     "start_time": "2016-09-30T22:00:13.170693"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def _mean_hiddens(rbm, v):\n",
    "    p = safe_sparse_dot(v, rbm.W.T) + rbm.h_bias\n",
    "    print \"mean hiddens W: \",np.linalg.norm(rbm.W.T, ord=2),\" v: \",np.linalg.norm(v, ord=2),\" h: \",np.linalg.norm(rbm.h_bias, ord=2),  \" p: \",   np.linalg.norm(p, ord=2)\n",
    "    return expit(p, out=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T22:00:14.996889",
     "start_time": "2016-09-30T22:00:14.993813"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mean_visibles(rbm, h):\n",
    "    p = np.dot(h, rbm.W) + rbm.v_bias\n",
    "    return expit(p, out=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T22:00:15.157314",
     "start_time": "2016-09-30T22:00:15.154359"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sample_visibles(rbm, h):\n",
    "    return sample_layer(rbm, _mean_visible(rbm, h))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:39.829999",
     "start_time": "2016-09-02T19:24:39.826110"
    },
    "collapsed": true
   },
   "source": [
    "def sample_visibles(rbm, h):\n",
    "    p = np.dot(h, rbm.W)\n",
    "    p += rbm.v_bias\n",
    "    expit(p, out=p)\n",
    "    return (rbm.rng.random_sample(size=p.shape) < p), p"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T20:12:06.667567",
     "start_time": "2016-09-02T20:12:06.663179"
    },
    "collapsed": true
   },
   "source": [
    "def sample_hiddens(rbm, v):\n",
    "    p = _mean_hiddens(rbm, v)\n",
    "    return (rbm.rng.random_sample(size=p.shape) < p), p "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:40.295848",
     "start_time": "2016-09-02T19:24:40.292563"
    },
    "collapsed": true
   },
   "source": [
    "def gibbs_simple(rbm, v):\n",
    "    h_ = _sample_hiddens(rbm, v)\n",
    "    v_ = _sample_visibles(rbm, h_)\n",
    "    return v_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:41.043974",
     "start_time": "2016-09-02T19:24:41.034838"
    },
    "collapsed": false
   },
   "source": [
    "# uses sample_x(), not _sample_x()\n",
    "# quite different from simple\n",
    "# we return all samples, where as before we just sampled the visible\n",
    "# and the h we return is the mean, not the sample\n",
    "def gibbs(rbm, vis, n_times=1):\n",
    "    v_pos = vis\n",
    "    h_samp, h_pos = sample_hiddens(rbm, v_pos)\n",
    "    h_neg = np.zeros_like(h_pos)\n",
    "    v_neg = np.zeros_like(v_pos)\n",
    "            \n",
    "    if (n_times > 0):               \n",
    "        v_neg = _sample_visibles(rbm, h_samp)\n",
    "        h_samp, h_neg = sample_hiddens(rbm, v_neg)\n",
    "        \n",
    "        for i in range(n_times-1):\n",
    "            v_neg = sample_visibles(rbm, h_samp)\n",
    "            h_samp, h_neg = sample_hiddens(rbm, v_neg)\n",
    "        end\n",
    "    end\n",
    "    return v_pos, h_pos, v_neg, h_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:22.990533",
     "start_time": "2016-09-30T23:25:22.983272"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# init starts with samples = [0] :| ?\n",
    "def init_batch(rbm, vis):\n",
    "    print \"init batch: \",np.linalg.norm(vis,ord=2)\n",
    "    v_pos = vis\n",
    "    v_init = v_pos\n",
    "    \n",
    "    # don't we sample, not just take the mean?\n",
    "    # NO, beause the jl code does\n",
    "    #     h_samples, h_pos = sample_hiddens(rbm,v_pos)\n",
    "    # and we never use h_samples\n",
    "\n",
    "    h_pos = _mean_hiddens(rbm, v_pos)\n",
    "    h_init = h_pos\n",
    "    \n",
    "    return v_pos, h_pos, v_init, h_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:23.306725",
     "start_time": "2016-09-30T23:25:23.299799"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(rbm, X):\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_batches = int(np.ceil(float(n_samples) / rbm.batch_size))\n",
    "    \n",
    "    print \"fitting with n_batches = \",n_batches, \" in \",rbm.n_iter, \"iterations\"\n",
    "    \n",
    "    batch_slices = list(gen_even_slices(n_batches * rbm.batch_size,\n",
    "                                        n_batches, n_samples))\n",
    "    for iter in xrange(1, rbm.n_iter + 1):\n",
    "        for batch_slice in batch_slices:\n",
    "            fit_batch(rbm, X[batch_slice])\n",
    "\n",
    "        print iter , \" . \" ,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run an RBM right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:23.627311",
     "start_time": "2016-09-30T23:25:23.620054"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def equilibrate(rbm, v0, h0, iters=3):\n",
    "    mv = v0\n",
    "    mh = h0\n",
    "    for i in range(iters):\n",
    "        mv = 0.5 *mv_update(rbm, mv, mh) + 0.5*mv\n",
    "        mh = 0.5 *mh_update(rbm, mv, mh) + 0.5*mh\n",
    "        print \"eq \",i, np.linalg.norm(mv, ord=2),np.linalg.norm(mh, ord=2)\n",
    "\n",
    "    return mv, mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:23.777034",
     "start_time": "2016-09-30T23:25:23.772038"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mv_update(rbm, v, h):  \n",
    "    a = np.dot(h, rbm.W) + rbm.v_bias\n",
    "\n",
    "    h_fluc = h-(h*h)\n",
    "    a += h_fluc.dot(rbm.W2)*(0.5-v)\n",
    "    \n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:23.933308",
     "start_time": "2016-09-30T23:25:23.928765"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_update(W2, v, h):\n",
    "    a = safe_sparse_dot(v, rbm.W.T) + rbm.h_bias\n",
    "    \n",
    "    v_fluc = v-(v*v)\n",
    "    a += v_fluc.dot(rbm.W2.T)*(0.5-h)\n",
    "\n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:24.091369",
     "start_time": "2016-09-30T23:25:24.082834"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_gradient(rbm, v_pos, h_pos ,v_neg, h_neg):\n",
    "    # naive  / mean field\n",
    "    dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "    \n",
    "    print \"dW naive\", np.linalg.norm(dW, ord=2)\n",
    "    # tap2 correction\n",
    "    h_fluc = (h_neg - (h_neg*h_neg)).T\n",
    "    v_fluc = (v_neg - (v_neg*v_neg))\n",
    "    dW_tap2 = h_fluc.dot(v_fluc)*rbm.W\n",
    "    print \"dW tap2\", np.linalg.norm(dW_tap2, ord=2)\n",
    "       \n",
    "    dW -= dW_tap2\n",
    "\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:24.292782",
     "start_time": "2016-09-30T23:25:24.243687"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_batch(rbm, X_batch):    \n",
    "    lr = float(rbm.learning_rate) / X_batch.shape[0]\n",
    "    decay = rbm.decay\n",
    "    \n",
    "    print X_batch.shape\n",
    "\n",
    "    print \"W, hb vb norm start batch\", np.linalg.norm(rbm.W, ord=2), np.linalg.norm(rbm.h_bias, ord=2), np.linalg.norm(rbm.v_bias, ord=2)\n",
    "    print \"batch norm \", np.linalg.norm(X_batch, ord=2)\n",
    "    \n",
    "    v_pos, h_pos, v_init, h_init = init_batch(rbm, X_batch)\n",
    " \n",
    "    print \"v, h init norm post init\", np.linalg.norm(v_init, ord=2), np.linalg.norm(h_init, ord=2)\n",
    "    print \"h pos norm \", np.linalg.norm(h_pos, ord=2)\n",
    "\n",
    "\n",
    "    # get_negative_samples\n",
    "    v_neg, h_neg = equilibrate(rbm, v_init, h_init, rbm.neq_steps) \n",
    "    print \"v, h neg norm post eq \", np.linalg.norm(v_neg, ord=2), np.linalg.norm(h_neg, ord=2)\n",
    "\n",
    "    # basic gradient\n",
    "    dW = weight_gradient(rbm, v_pos, h_pos ,v_neg, h_neg) \n",
    "    \n",
    "    print \"dW norm \", np.linalg.norm(dW, ord=2)\n",
    "\n",
    "\n",
    "    # regularization based on weight decay\n",
    "    #  similar to momentum >\n",
    "    if rbm.weight_decay == \"L2\":\n",
    "        dW += decay * np.sign(rbm.W)\n",
    "        print \" dW decay L2 \", np.linalg.norm(dW, ord=2)\n",
    "    elif rbm.weight_decay == \"L1\":\n",
    "        dW += decay * rbm.W\n",
    "        print \" dW decay L1 \", np.linalg.norm(dW, ord=2)\n",
    "\n",
    "\n",
    "    # can we use BLAS here ?\n",
    "    # momentum\n",
    "    # note:  what do we do if lr changes per step ?    \n",
    "    dW += rbm.momentum * rbm.dW_prev  \n",
    "    print \" dW mom \", np.linalg.norm(dW, ord=2)\n",
    "\n",
    "    rbm.W += lr * dW \n",
    "    \n",
    "    # for next iteration\n",
    "    rbm.dW_prev =  dW  \n",
    "    rbm.W2 = rbm.W*rbm.W\n",
    "    \n",
    "    # update bias terms\n",
    "    rbm.h_bias += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "    rbm.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0))\n",
    "\n",
    "    print \" h_bias \", np.linalg.norm(rbm.h_bias, ord=2)\n",
    "    print \" v_bias \", np.linalg.norm(rbm.v_bias, ord=2)\n",
    "\n",
    "    # only resample (binomial) for CD\n",
    "    # h_neg[rbm.rng.uniform(size=h_neg.shape) < h_neg] = 1.0  \n",
    "    # rbm.h_samples_ = np.floor(h_neg, h_neg)\n",
    "   \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:24.439968",
     "start_time": "2016-09-30T23:25:24.388390"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W emf_init norm  4.4166204107e-19\n",
      "(50000, 784)\n",
      "12.8840987267\n",
      "mean X 8.43148429713\n",
      "norm probVis 8.43148429713\n",
      "v bias emf_init shape  (784,)\n",
      "v bias emf_init norm  195.129720728\n",
      "h bias emf_init norm  0.0\n"
     ]
    }
   ],
   "source": [
    "rbm = EMF_RBM(momentum=0.5, decay=0.01, learning_rate=0.005, n_iter=20, sigma=1e-20, neq_steps=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-04T23:28:03.944844",
     "start_time": "2016-09-04T23:28:03.918453"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "fit_batch(rbm, X[0:100])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T23:25:29.774338",
     "start_time": "2016-09-30T23:25:24.676160"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting with n_batches =  500  in  20 iterations\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 4.4166204107e-19 0.0 195.129720728\n",
      "batch norm  87.0370328792\n",
      "init batch:  87.0370328792\n",
      "mean hiddens W:  4.4166204107e-19  v:  87.0370328792  h:  0.0  p:  1.29083230785e-17\n",
      "v, h init norm post init 87.0370328792 80.0\n",
      "h pos norm  80.0\n",
      "eq  0 84.8274299932 80.0\n",
      "eq  1 84.3616828058 80.0\n",
      "eq  2 84.2861684011 80.0\n",
      "v, h neg norm post eq  84.2861684011 80.0\n",
      "dW naive 815.269253604\n",
      "dW tap2 1.93201777067e-18\n",
      "dW norm  815.269253604\n",
      " dW decay L1  815.269253604\n",
      " dW mom  815.269253604\n",
      " h_bias  0.0\n",
      " v_bias  195.129872726\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.0407634626802 0.0 195.129872726\n",
      "batch norm  85.20559568\n",
      "init batch:  85.20559568\n",
      "mean hiddens W:  0.0407634626802  v:  85.20559568  h:  0.0  p:  0.676443733537\n",
      "v, h init norm post init 85.20559568 79.9859744459\n",
      "h pos norm  79.9859744459\n",
      "eq  0 84.1718219401 79.9792969861\n",
      "eq  1 84.0899280751 79.9726415642\n",
      "eq  2 84.1567041854 79.9676643817\n",
      "v, h neg norm post eq  84.1567041854 79.9676643817\n",
      "dW naive 609.958778814\n",
      "dW tap2 0.216172813234\n",
      "dW norm  609.894473281\n",
      " dW decay L1  609.894596067\n",
      " dW mom  829.404035464\n",
      " h_bias  9.07494912485e-06\n",
      " v_bias  195.129966573\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.0761049646757 9.07494912485e-06 195.129966573\n",
      "batch norm  84.8736785397\n",
      "init batch:  84.8736785397\n",
      "mean hiddens W:  0.0761049646757  v:  84.8736785397  h:  9.07494912485e-06  p:  1.59069903597\n",
      "v, h init norm post init 84.8736785397 79.8113432497\n",
      "h pos norm  79.8113432497\n",
      "eq  0 83.8566063436 79.8207521491\n",
      "eq  1 83.8777483951 79.8302579601\n",
      "eq  2 84.0053494396 79.8374512388\n",
      "v, h neg norm post eq  84.0053494396 79.8374512388\n",
      "dW naive 753.187429948\n",
      "dW tap2 0.405820566056\n",
      "dW norm  753.243924326\n",
      " dW decay L1  753.243785813\n",
      " dW mom  820.567735093\n",
      " h_bias  4.32458070845e-06\n",
      " v_bias  195.129849699\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.096731557914 4.32458070845e-06 195.129849699\n",
      "batch norm  85.4617553633\n",
      "init batch:  85.4617553633\n",
      "mean hiddens W:  0.096731557914  v:  85.4617553633  h:  4.32458070845e-06  p:  2.05666706087\n",
      "v, h init norm post init 85.4617553633 79.6753959505\n",
      "h pos norm  79.6753959505\n",
      "eq  0 84.0174983265 79.6714397173\n",
      "eq  1 83.8481993495 79.6676078348\n",
      "eq  2 83.8921833428 79.6648151755\n",
      "v, h neg norm post eq  83.8921833428 79.6648151755\n",
      "dW naive 833.632431575\n",
      "dW tap2 0.521010323716\n",
      "dW norm  833.6329642\n",
      " dW decay L1  833.633017852\n",
      " dW mom  1030.80679545\n",
      " h_bias  5.1759405635e-07\n",
      " v_bias  195.130083795\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.122820365262 5.1759405635e-07 195.130083795\n",
      "batch norm  83.0717878752\n",
      "init batch:  83.0717878752\n",
      "mean hiddens W:  0.122820365262  v:  83.0717878752  h:  5.1759405635e-07  p:  2.89407607756\n",
      "v, h init norm post init 83.0717878752 79.5500941145\n",
      "h pos norm  79.5500941145\n",
      "eq  0 82.7455709684 79.5323278173\n",
      "eq  1 83.1362342993 79.5148107772\n",
      "eq  2 83.4568100137 79.5018344644\n",
      "v, h neg norm post eq  83.4568100137 79.5018344644\n",
      "dW naive 696.713273533\n",
      "dW tap2 0.635515030806\n",
      "dW norm  696.555316469\n",
      " dW decay L1  696.55560161\n",
      " dW mom  942.360986483\n",
      " h_bias  2.37426041833e-05\n",
      " v_bias  195.129975834\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.153242478179 2.37426041833e-05 195.129975834\n",
      "batch norm  84.9058170044\n",
      "init batch:  84.9058170044\n",
      "mean hiddens W:  0.153242478179  v:  84.9058170044  h:  2.37426041833e-05  p:  3.89112435012\n",
      "v, h init norm post init 84.9058170044 79.3677597297\n",
      "h pos norm  79.3677597297\n",
      "eq  0 83.3602672153 79.3134767795\n",
      "eq  1 83.2420920288 79.2596093214\n",
      "eq  2 83.3415082582 79.2194718706\n",
      "v, h neg norm post eq  83.3415082582 79.2194718706\n",
      "dW naive 1061.35216323\n",
      "dW tap2 0.788030939579\n",
      "dW norm  1061.04833854\n",
      " dW decay L1  1061.04890542\n",
      " dW mom  1328.1855061\n",
      " h_bias  9.63471628207e-05\n",
      " v_bias  195.129946482\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.198034316883 9.63471628207e-05 195.129946482\n",
      "batch norm  85.8259216881\n",
      "init batch:  85.8259216881\n",
      "mean hiddens W:  0.198034316883  v:  85.8259216881  h:  9.63471628207e-05  p:  5.41148758259\n",
      "v, h init norm post init 85.8259216881 79.1255841016\n",
      "h pos norm  79.1255841016\n",
      "eq  0 83.801153001 79.0647212069\n",
      "eq  1 83.36682871 79.0046693545\n",
      "eq  2 83.2834971521 78.9601452304\n",
      "v, h neg norm post eq  83.2834971521 78.9601452304\n",
      "dW naive 787.397263873\n",
      "dW tap2 1.02781146379\n",
      "dW norm  787.019085458\n",
      " dW decay L1  787.019941176\n",
      " dW mom  1286.8603843\n",
      " h_bias  0.000176067265528\n",
      " v_bias  195.12998184\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.24528466232 0.000176067265528 195.12998184\n",
      "batch norm  92.6005445273\n",
      "init batch:  92.6005445273\n",
      "mean hiddens W:  0.24528466232  v:  92.6005445273  h:  0.000176067265528  p:  6.81102588186\n",
      "v, h init norm post init 92.6005445273 78.8112446968\n",
      "h pos norm  78.8112446968\n",
      "eq  0 87.0973585593 78.8008171442\n",
      "eq  1 84.9400718863 78.7915629523\n",
      "eq  2 84.0084467852 78.7853860166\n",
      "v, h neg norm post eq  84.0084467852 78.7853860166\n",
      "dW naive 927.167106699\n",
      "dW tap2 1.26330497063\n",
      "dW norm  927.182415776\n",
      " dW decay L1  927.182546947\n",
      " dW mom  1241.01500002\n",
      " h_bias  0.000184816945976\n",
      " v_bias  195.129925852\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.278880944283 0.000184816945976 195.129925852\n",
      "batch norm  88.8350681982\n",
      "init batch:  88.8350681982\n",
      "mean hiddens W:  0.278880944283  v:  88.8350681982  h:  0.000184816945976  p:  6.13144268314\n",
      "v, h init norm post init 88.8350681982 79.4066355747\n",
      "h pos norm  79.4066355747\n",
      "eq  0 85.6227040898 79.3524020306\n",
      "eq  1 84.438140058 79.2996764012\n",
      "eq  2 83.949584996 79.2610970145\n",
      "v, h neg norm post eq  83.949584996 79.2610970145\n",
      "dW naive 684.661913017\n",
      "dW tap2 1.40117868899\n",
      "dW norm  684.280807907\n",
      " dW decay L1  684.281700242\n",
      " dW mom  1137.05593249\n",
      " h_bias  0.000251965955667\n",
      " v_bias  195.129883201\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.312798816888 0.000251965955667 195.129883201\n",
      "batch norm  88.0535831328\n",
      "init batch:  88.0535831328\n",
      "mean hiddens W:  0.312798816888  v:  88.0535831328  h:  0.000251965955667  p:  7.909525246\n",
      "v, h init norm post init 88.0535831328 80.1378864481\n",
      "h pos norm  80.1378864481\n",
      "eq  0 85.2669241573 80.0196907974\n",
      "eq  1 84.421534955 79.9043231204\n",
      "eq  2 84.1305185974 79.8196008717\n",
      "v, h neg norm post eq  84.1305185974 79.8196008717\n",
      "dW naive 821.987553255\n",
      "dW tap2 1.54899031479\n",
      "dW norm  821.210545069\n",
      " dW decay L1  821.212190747\n",
      " dW mom  1240.38078784\n",
      " h_bias  0.000400168577552\n",
      " v_bias  195.12977646\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.356166228696 0.000400168577552 195.12977646\n",
      "batch norm  81.2694101869\n",
      "init batch:  81.2694101869\n",
      "mean hiddens W:  0.356166228696  v:  81.2694101869  h:  0.000400168577552  p:  7.45137984251\n",
      "v, h init norm post init 81.2694101869 80.4354053841\n",
      "h pos norm  80.4354053841\n",
      "eq  0 82.0063810196 80.3478646145\n",
      "eq  1 82.9504587586 80.2626968081\n",
      "eq  2 83.5575880857 80.2003219859\n",
      "v, h neg norm post eq  83.5575880857 80.2003219859\n",
      "dW naive 832.069310117\n",
      "dW tap2 1.75101256856\n",
      "dW norm  831.583182926\n",
      " dW decay L1  831.584396929\n",
      " dW mom  1074.01386999\n",
      " h_bias  0.000508490320421\n",
      " v_bias  195.129834254\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.39530050397 0.000508490320421 195.129834254\n",
      "batch norm  85.2398193507\n",
      "init batch:  85.2398193507\n",
      "mean hiddens W:  0.39530050397  v:  85.2398193507  h:  0.000508490320421  p:  8.05454337207\n",
      "v, h init norm post init 85.2398193507 79.8873465635\n",
      "h pos norm  79.8873465635\n",
      "eq  0 83.9169713433 79.8924176339\n",
      "eq  1 83.806063189 79.9006336969\n",
      "eq  2 83.8881262616 79.9088797211\n",
      "v, h neg norm post eq  83.8881262616 79.9088797211\n",
      "dW naive 622.040453217\n",
      "dW tap2 1.92555718268\n",
      "dW norm  622.123642035\n",
      " dW decay L1  622.123589233\n",
      " dW mom  885.168117538\n",
      " h_bias  0.000486365878656\n",
      " v_bias  195.129896868\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.417179596593 0.000486365878656 195.129896868\n",
      "batch norm  100.480133339\n",
      "init batch:  100.480133339\n",
      "mean hiddens W:  0.417179596593  v:  100.480133339  h:  0.000486365878656  p:  8.58307357257\n",
      "v, h init norm post init 100.480133339 78.9217981174\n",
      "h pos norm  78.9217981174\n",
      "eq  0 91.327034436 79.1162746321\n",
      "eq  1 87.3531983784 79.3139064324\n",
      "eq  2 85.543877916 79.4644191246\n",
      "v, h neg norm post eq  85.543877916 79.4644191246\n",
      "dW naive 1227.64197019\n",
      "dW tap2 2.02515984868\n",
      "dW norm  1228.57449275\n",
      " dW decay L1  1228.57275883\n",
      " dW mom  1192.68341723\n",
      " h_bias  0.000205388626086\n",
      " v_bias  195.13014114\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.40762752539 0.000205388626086 195.13014114\n",
      "batch norm  98.7923187571\n",
      "init batch:  98.7923187571\n",
      "mean hiddens W:  0.40762752539  v:  98.7923187571  h:  0.000205388626086  p:  8.83786584826\n",
      "v, h init norm post init 98.7923187571 80.0130665981\n",
      "h pos norm  80.0130665981\n",
      "eq  0 90.9862783368 80.1243072614\n",
      "eq  1 87.5893626993 80.239577863\n",
      "eq  2 86.0361679398 80.3288013535\n",
      "v, h neg norm post eq  86.0361679398 80.3288013535\n",
      "dW naive 1050.58978433\n",
      "dW tap2 1.94812752875\n",
      "dW norm  1051.137834\n",
      " dW decay L1  1051.13666886\n",
      " dW mom  1518.45978651\n",
      " h_bias  3.38028032242e-05\n",
      " v_bias  195.130191391\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.396883014805 3.38028032242e-05 195.130191391\n",
      "batch norm  85.3784372864\n",
      "init batch:  85.3784372864\n",
      "mean hiddens W:  0.396883014805  v:  85.3784372864  h:  3.38028032242e-05  p:  11.8323271228\n",
      "v, h init norm post init 85.3784372864 81.8428846167\n",
      "h pos norm  81.8428846167\n",
      "eq  0 84.8685563136 81.7996893624\n",
      "eq  1 85.1241558826 81.7603984089\n",
      "eq  2 85.3763938987 81.7334954511\n",
      "v, h neg norm post eq  85.3763938987 81.7334954511\n",
      "dW naive 811.248527526\n",
      "dW tap2 1.88341207982\n",
      "dW norm  810.900379719\n",
      " dW decay L1  810.901050124\n",
      " dW mom  854.249857625\n",
      " h_bias  7.34885289009e-05\n",
      " v_bias  195.130039486\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.404149722455 7.34885289009e-05 195.130039486\n",
      "batch norm  83.6326111708\n",
      "init batch:  83.6326111708\n",
      "mean hiddens W:  0.404149722455  v:  83.6326111708  h:  7.34885289009e-05  p:  13.0899152967\n",
      "v, h init norm post init 83.6326111708 82.2169017272\n",
      "h pos norm  82.2169017272\n",
      "eq  0 84.3029638806 82.1850398048\n",
      "eq  1 85.0587179376 82.157417172\n",
      "eq  2 85.5346879968 82.1394972507\n",
      "v, h neg norm post eq  85.5346879968 82.1394972507\n",
      "dW naive 733.389219496\n",
      "dW tap2 1.93391837587\n",
      "dW norm  733.19879436\n",
      " dW decay L1  733.199364887\n",
      " dW mom  829.72330166\n",
      " h_bias  9.59610788204e-05\n",
      " v_bias  195.129975853\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.416009486268 9.59610788204e-05 195.129975853\n",
      "batch norm  81.5593200617\n",
      "init batch:  81.5593200617\n",
      "mean hiddens W:  0.416009486268  v:  81.5593200617  h:  9.59610788204e-05  p:  7.4087623543\n",
      "v, h init norm post init 81.5593200617 81.0605263245\n",
      "h pos norm  81.0605263245\n",
      "eq  0 82.9670266095 81.2900211273\n",
      "eq  1 84.2729914133 81.5217304413\n",
      "eq  2 85.059482689 81.6972364624\n",
      "v, h neg norm post eq  85.059482689 81.6972364624\n",
      "dW naive 1021.34612591\n",
      "dW tap2 2.00138924206\n",
      "dW norm  1022.69917938\n",
      " dW decay L1  1022.69658924\n",
      " dW mom  1060.16415158\n",
      " h_bias  0.00022887379391\n",
      " v_bias  195.130004768\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.393838262117 0.00022887379391 195.130004768\n",
      "batch norm  85.0966340008\n",
      "init batch:  85.0966340008\n",
      "mean hiddens W:  0.393838262117  v:  85.0966340008  h:  0.00022887379391  p:  7.25790172949\n",
      "v, h init norm post init 85.0966340008 80.556886262\n",
      "h pos norm  80.556886262\n",
      "eq  0 84.3850824831 80.7325685795\n",
      "eq  1 84.6598302536 80.9108656916\n",
      "eq  2 84.9405814916 81.046485567\n",
      "v, h neg norm post eq  84.9405814916 81.046485567\n",
      "dW naive 902.484821399\n",
      "dW tap2 1.86873327423\n",
      "dW norm  903.676051508\n",
      " dW decay L1  903.673846614\n",
      " dW mom  1261.21636615\n",
      " h_bias  0.000482058117843\n",
      " v_bias  195.130018277\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.362749947951 0.000482058117843 195.130018277\n",
      "batch norm  85.5468256658\n",
      "init batch:  85.5468256658\n",
      "mean hiddens W:  0.362749947951  v:  85.5468256658  h:  0.000482058117843  p:  5.75958100883\n",
      "v, h init norm post init 85.5468256658 80.1438668664\n",
      "h pos norm  80.1438668664\n",
      "eq  0 84.4456473159 80.2568437657\n",
      "eq  1 84.4304085375 80.3715769287\n",
      "eq  2 84.5509606304 80.4588761596\n",
      "v, h neg norm post eq  84.5509606304 80.4588761596\n",
      "dW naive 675.03739478\n",
      "dW tap2 1.67143843655\n",
      "dW norm  676.063590057\n",
      " dW decay L1  676.061716726\n",
      " dW mom  1085.99598053\n",
      " h_bias  0.000645338205458\n",
      " v_bias  195.130046769\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.334811853191 0.000645338205458 195.130046769\n",
      "batch norm  91.3462717747\n",
      "init batch:  91.3462717747\n",
      "mean hiddens W:  0.334811853191  v:  91.3462717747  h:  0.000645338205458  p:  5.78835843764\n",
      "v, h init norm post init 91.3462717747 79.5803171955\n",
      "h pos norm  79.5803171955\n",
      "eq  0 87.0188547895 79.7045629556\n",
      "eq  1 85.4655334497 79.8304757911\n",
      "eq  2 84.8456957216 79.9260834529\n",
      "v, h neg norm post eq  84.8456957216 79.9260834529\n",
      "dW naive 902.366799364\n",
      "dW tap2 1.49080257539\n",
      "dW norm  903.154659993\n",
      " dW decay L1  903.153135114\n",
      " dW mom  1179.37007794\n",
      " h_bias  0.000823618604776\n",
      " v_bias  195.130188248\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.30583374049 0.000823618604776 195.130188248\n",
      "batch norm  92.4510553234\n",
      "init batch:  92.4510553234\n",
      "mean hiddens W:  0.30583374049  v:  92.4510553234  h:  0.000823618604776  p:  4.87169711383\n",
      "v, h init norm post init 92.4510553234 79.601969094\n",
      "h pos norm  79.601969094\n",
      "eq  0 87.6691653383 79.7145003412\n",
      "eq  1 85.8184273711 79.8281871243\n",
      "eq  2 85.0370586145 79.9142665236\n",
      "v, h neg norm post eq  85.0370586145 79.9142665236\n",
      "dW naive 871.697483144\n",
      "dW tap2 1.30686854698\n",
      "dW norm  872.351801034\n",
      " dW decay L1  872.350368666\n",
      " dW mom  1302.71225261\n",
      " h_bias  0.000983497824494\n",
      " v_bias  195.130356176\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.278815322825 0.000983497824494 195.130356176\n",
      "batch norm  87.5706909703\n",
      "init batch:  87.5706909703\n",
      "mean hiddens W:  0.278815322825  v:  87.5706909703  h:  0.000983497824494  p:  4.01053394626\n",
      "v, h init norm post init 87.5706909703 79.8973749662\n",
      "h pos norm  79.8973749662\n",
      "eq  0 85.2867991898 80.0199751926\n",
      "eq  1 84.7929680094 80.1434512728\n",
      "eq  2 84.698356487 80.2366823111\n",
      "v, h neg norm post eq  84.698356487 80.2366823111\n",
      "dW naive 879.211488845\n",
      "dW tap2 1.15689171487\n",
      "dW norm  880.007913368\n",
      " dW decay L1  880.006353576\n",
      " dW mom  1339.70045294\n",
      " h_bias  0.00115595413351\n",
      " v_bias  195.130464396\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.250443427371 0.00115595413351 195.130464396\n",
      "batch norm  90.2455408042\n",
      "init batch:  90.2455408042\n",
      "mean hiddens W:  0.250443427371  v:  90.2455408042  h:  0.00115595413351  p:  3.54277717924\n",
      "v, h init norm post init 90.2455408042 80.0102157638\n",
      "h pos norm  80.0102157638\n",
      "eq  0 86.6753257372 80.1305314118\n",
      "eq  1 85.5389593539 80.2515324714\n",
      "eq  2 85.1318504022 80.3427689966\n",
      "v, h neg norm post eq  85.1318504022 80.3427689966\n",
      "dW naive 1180.86202418\n",
      "dW tap2 0.989920801419\n",
      "dW norm  1181.36317642\n",
      " dW decay L1  1181.36203301\n",
      " dW mom  1707.05640567\n",
      " h_bias  0.00132444127732\n",
      " v_bias  195.130399037\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.226350793816 0.00132444127732 195.130399037\n",
      "batch norm  85.122411387\n",
      "init batch:  85.122411387\n",
      "mean hiddens W:  0.226350793816  v:  85.122411387  h:  0.00132444127732  p:  4.03157519663\n",
      "v, h init norm post init 85.122411387 80.5970272758\n",
      "h pos norm  80.5970272758\n",
      "eq  0 84.3505048346 80.6168165497\n",
      "eq  1 84.5189643067 80.6371285513\n",
      "eq  2 84.7291568672 80.6527104206\n",
      "v, h neg norm post eq  84.7291568672 80.6527104206\n",
      "dW naive 718.881472584\n",
      "dW tap2 0.878680385968\n",
      "dW norm  719.098683848\n",
      " dW decay L1  719.098375842\n",
      " dW mom  763.798395364\n",
      " h_bias  0.00135414064479\n",
      " v_bias  195.130269808\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.219948292993 0.00135414064479 195.130269808\n",
      "batch norm  87.8567662809\n",
      "init batch:  87.8567662809\n",
      "mean hiddens W:  0.219948292993  v:  87.8567662809  h:  0.00135414064479  p:  3.48508196903\n",
      "v, h init norm post init 87.8567662809 80.5838589132\n",
      "h pos norm  80.5838589132\n",
      "eq  0 85.542418224 80.5845301707\n",
      "eq  1 85.0330447624 80.5855257233\n",
      "eq  2 84.9299302097 80.5864858508\n",
      "v, h neg norm post eq  84.9299302097 80.5864858508\n",
      "dW naive 1089.28508594\n",
      "dW tap2 0.849311552513\n",
      "dW norm  1089.37956129\n",
      " dW decay L1  1089.37955586\n",
      " dW mom  1075.88317339\n",
      " h_bias  0.00135663240328\n",
      " v_bias  195.130223086\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.224749736054 0.00135663240328 195.130223086\n",
      "batch norm  91.6108638141\n",
      "init batch:  91.6108638141\n",
      "mean hiddens W:  0.224749736054  v:  91.6108638141  h:  0.00135663240328  p:  4.12171242078\n",
      "v, h init norm post init 91.6108638141 80.6525160969\n",
      "h pos norm  80.6525160969\n",
      "eq  0 87.4939377741 80.6213909738\n",
      "eq  1 85.9939873445 80.5907426352\n",
      "eq  2 85.3814774524 80.5680634037\n",
      "v, h neg norm post eq  85.3814774524 80.5680634037\n",
      "dW naive 800.371075805\n",
      "dW tap2 0.844897024154\n",
      "dW norm  800.243577535\n",
      " dW decay L1  800.244023682\n",
      " dW mom  1141.97116572\n",
      " h_bias  0.00131619748209\n",
      " v_bias  195.13010333\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.244657893925 0.00131619748209 195.13010333\n",
      "batch norm  88.3227792232\n",
      "init batch:  88.3227792232\n",
      "mean hiddens W:  0.244657893925  v:  88.3227792232  h:  0.00131619748209  p:  4.74975092092\n",
      "v, h init norm post init 88.3227792232 80.7683639331\n",
      "h pos norm  80.7683639331\n",
      "eq  0 86.0668527943 80.7808466212\n",
      "eq  1 85.4269394781 80.7939683826\n",
      "eq  2 85.2247237819 80.8042340522\n",
      "v, h neg norm post eq  85.2247237819 80.8042340522\n",
      "dW naive 623.681118346\n",
      "dW tap2 0.9375024999\n",
      "dW norm  623.821740173\n",
      " dW decay L1  623.821519277\n",
      " dW mom  828.365525995\n",
      " h_bias  0.00133644099968\n",
      " v_bias  195.130125747\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.257871353155 0.00133644099968 195.130125747\n",
      "batch norm  83.9309935291\n",
      "init batch:  83.9309935291\n",
      "mean hiddens W:  0.257871353155  v:  83.9309935291  h:  0.00133644099968  p:  4.84645356476\n",
      "v, h init norm post init 83.9309935291 80.4621185462\n",
      "h pos norm  80.4621185462\n",
      "eq  0 83.734958647 80.6030271717\n",
      "eq  1 84.3023997392 80.7449985167\n",
      "eq  2 84.7404786496 80.8522181408\n",
      "v, h neg norm post eq  84.7404786496 80.8522181408\n",
      "dW naive 935.570175065\n",
      "dW tap2 1.00805243875\n",
      "dW norm  936.2800069\n",
      " dW decay L1  936.278302172\n",
      " dW mom  837.720755825\n",
      " h_bias  0.00153502571881\n",
      " v_bias  195.129831569\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.237631874252 0.00153502571881 195.129831569\n",
      "batch norm  88.3250753716\n",
      "init batch:  88.3250753716\n",
      "mean hiddens W:  0.237631874252  v:  88.3250753716  h:  0.00153502571881  p:  4.46814870969\n",
      "v, h init norm post init 88.3250753716 80.6063623856\n",
      "h pos norm  80.6063623856\n",
      "eq  0 85.8490766125 80.6383653244\n",
      "eq  1 85.2371387849 80.6710606313\n",
      "eq  2 85.0737043487 80.69604727\n",
      "v, h neg norm post eq  85.0737043487 80.69604727\n",
      "dW naive 983.005283061\n",
      "dW tap2 0.905532909688\n",
      "dW norm  983.123876224\n",
      " dW decay L1  983.123511394\n",
      " dW mom  1200.5707674\n",
      " h_bias  0.00158234696539\n",
      " v_bias  195.129515099\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.228797913482 0.00158234696539 195.129515099\n",
      "batch norm  86.7714209422\n",
      "init batch:  86.7714209422\n",
      "mean hiddens W:  0.228797913482  v:  86.7714209422  h:  0.00158234696539  p:  3.99017965232\n",
      "v, h init norm post init 86.7714209422 80.5895371076\n",
      "h pos norm  80.5895371076\n",
      "eq  0 85.0351535202 80.5952577882\n",
      "eq  1 84.7554842056 80.6014790448\n",
      "eq  2 84.7576230423 80.6064767212\n",
      "v, h neg norm post eq  84.7576230423 80.6064767212\n",
      "dW naive 746.828961859\n",
      "dW tap2 0.884427537571\n",
      "dW norm  746.966506269\n",
      " dW decay L1  746.966424003\n",
      " dW mom  735.718190468\n",
      " h_bias  0.00159264075537\n",
      " v_bias  195.129742928\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.229844976081 0.00159264075537 195.129742928\n",
      "batch norm  86.2615452678\n",
      "init batch:  86.2615452678\n",
      "mean hiddens W:  0.229844976081  v:  86.2615452678  h:  0.00159264075537  p:  4.16812749919\n",
      "v, h init norm post init 86.2615452678 80.5586528887\n",
      "h pos norm  80.5586528887\n",
      "eq  0 84.9926888652 80.5386682577\n",
      "eq  1 84.7382386335 80.5192709575\n",
      "eq  2 84.7031973177 80.5051052896\n",
      "v, h neg norm post eq  84.7031973177 80.5051052896\n",
      "dW naive 700.272580028\n",
      "dW tap2 0.887981279817\n",
      "dW norm  700.118424848\n",
      " dW decay L1  700.118754955\n",
      " dW mom  799.110168601\n",
      " h_bias  0.00156804640518\n",
      " v_bias  195.129807448\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.240110473419 0.00156804640518 195.129807448\n",
      "batch norm  84.3426711357\n",
      "init batch:  84.3426711357\n",
      "mean hiddens W:  0.240110473419  v:  84.3426711357  h:  0.00156804640518  p:  4.31068194862\n",
      "v, h init norm post init 84.3426711357 80.4934475318\n",
      "h pos norm  80.4934475318\n",
      "eq  0 83.9112063117 80.4698603327\n",
      "eq  1 84.133324018 80.4469644328\n",
      "eq  2 84.3519259596 80.4302430771\n",
      "v, h neg norm post eq  84.3519259596 80.4302430771\n",
      "dW naive 692.563129634\n",
      "dW tap2 0.95310349906\n",
      "dW norm  692.429602475\n",
      " dW decay L1  692.42999621\n",
      " dW mom  916.855161382\n",
      " h_bias  0.00153902913662\n",
      " v_bias  195.130011978\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.256303231213 0.00153902913662 195.130011978\n",
      "batch norm  83.5734009838\n",
      "init batch:  83.5734009838\n",
      "mean hiddens W:  0.256303231213  v:  83.5734009838  h:  0.00153902913662  p:  3.70171815235\n",
      "v, h init norm post init 83.5734009838 80.1278352212\n",
      "h pos norm  80.1278352212\n",
      "eq  0 83.254605477 80.1437057046\n",
      "eq  1 83.6627756566 80.1602397268\n",
      "eq  2 84.0080984462 80.1730819167\n",
      "v, h neg norm post eq  84.0080984462 80.1730819167\n",
      "dW naive 994.555850297\n",
      "dW tap2 1.0439345937\n",
      "dW norm  994.695147633\n",
      " dW decay L1  994.69497247\n",
      " dW mom  1213.74024674\n",
      " h_bias  0.00156401899223\n",
      " v_bias  195.130312558\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.269632875083 0.00156401899223 195.130312558\n",
      "batch norm  85.095495186\n",
      "init batch:  85.095495186\n",
      "mean hiddens W:  0.269632875083  v:  85.095495186  h:  0.00156401899223  p:  4.63786254651\n",
      "v, h init norm post init 85.095495186 79.8164020632\n",
      "h pos norm  79.8164020632\n",
      "eq  0 83.9633425226 79.8152656644\n",
      "eq  1 83.8829290821 79.8151565977\n",
      "eq  2 83.9632050458 79.8157486704\n",
      "v, h neg norm post eq  83.9632050458 79.8157486704\n",
      "dW naive 747.30309778\n",
      "dW tap2 1.11110705554\n",
      "dW norm  747.254697598\n",
      " dW decay L1  747.254724472\n",
      " dW mom  1113.00060373\n",
      " h_bias  0.00156738312829\n",
      " v_bias  195.130320412\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.285212882158 0.00156738312829 195.130320412\n",
      "batch norm  81.3394892491\n",
      "init batch:  81.3394892491\n",
      "mean hiddens W:  0.285212882158  v:  81.3394892491  h:  0.00156738312829  p:  4.75681532384\n",
      "v, h init norm post init 81.3394892491 79.5780817335\n",
      "h pos norm  79.5780817335\n",
      "eq  0 81.952739023 79.5654952738\n",
      "eq  1 82.7439247029 79.5538728842\n",
      "eq  2 83.2618014837 79.5457827288\n",
      "v, h neg norm post eq  83.2618014837 79.5457827288\n",
      "dW naive 947.916521239\n",
      "dW tap2 1.20336098772\n",
      "dW norm  947.915502042\n",
      " dW decay L1  947.915656059\n",
      " dW mom  1336.40680322\n",
      " h_bias  0.00155471334134\n",
      " v_bias  195.130554995\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.305191830363 0.00155471334134 195.130554995\n",
      "batch norm  84.0258211123\n",
      "init batch:  84.0258211123\n",
      "mean hiddens W:  0.305191830363  v:  84.0258211123  h:  0.00155471334134  p:  8.40714954727\n",
      "v, h init norm post init 84.0258211123 78.7541949909\n",
      "h pos norm  78.7541949909\n",
      "eq  0 82.9535521791 78.8224847313\n",
      "eq  1 83.0156848557 78.8931322254\n",
      "eq  2 83.1826809405 78.9476913144\n",
      "v, h neg norm post eq  83.1826809405 78.9476913144\n",
      "dW naive 734.689815296\n",
      "dW tap2 1.32136098826\n",
      "dW norm  735.074832819\n",
      " dW decay L1  735.073840217\n",
      " dW mom  1002.4855093\n",
      " h_bias  0.00165951930156\n",
      " v_bias  195.130243349\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.310625973174 0.00165951930156 195.130243349\n",
      "batch norm  90.5084130289\n",
      "init batch:  90.5084130289\n",
      "mean hiddens W:  0.310625973174  v:  90.5084130289  h:  0.00165951930156  p:  10.3189372687\n",
      "v, h init norm post init 90.5084130289 77.8768114548\n",
      "h pos norm  77.8768114548\n",
      "eq  0 85.8326634923 78.055241831\n",
      "eq  1 84.2108746326 78.2356035664\n",
      "eq  2 83.5708684117 78.3722255638\n",
      "v, h neg norm post eq  83.5708684117 78.3722255638\n",
      "dW naive 1014.80693595\n",
      "dW tap2 1.38956912192\n",
      "dW norm  1015.69865307\n",
      " dW decay L1  1015.69677356\n",
      " dW mom  1020.26339184\n",
      " h_bias  0.00191325314513\n",
      " v_bias  195.129846344\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.287976010332 0.00191325314513 195.129846344\n",
      "batch norm  85.7467378987\n",
      "init batch:  85.7467378987\n",
      "mean hiddens W:  0.287976010332  v:  85.7467378987  h:  0.00191325314513  p:  9.77551396054\n",
      "v, h init norm post init 85.7467378987 77.9646894609\n",
      "h pos norm  77.9646894609\n",
      "eq  0 83.2565702586 78.1156652266\n",
      "eq  1 82.8758316752 78.2682572425\n",
      "eq  2 82.8811717644 78.383821868\n",
      "v, h neg norm post eq  82.8811717644 78.383821868\n",
      "dW naive 1107.49828132\n",
      "dW tap2 1.2829780431\n",
      "dW norm  1108.18736356\n",
      " dW decay L1  1108.18590499\n",
      " dW mom  1430.83890722\n",
      " h_bias  0.00212795823471\n",
      " v_bias  195.129561629\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.257695079911 0.00212795823471 195.129561629\n",
      "batch norm  84.0323718544\n",
      "init batch:  84.0323718544\n",
      "mean hiddens W:  0.257695079911  v:  84.0323718544  h:  0.00212795823471  p:  7.41650133854\n",
      "v, h init norm post init 84.0323718544 78.3652920935\n",
      "h pos norm  78.3652920935\n",
      "eq  0 82.9274666958 78.3749387282\n",
      "eq  1 82.7648193849 78.3852016267\n",
      "eq  2 82.7750717948 78.393305488\n",
      "v, h neg norm post eq  82.7750717948 78.393305488\n",
      "dW naive 633.083199464\n",
      "dW tap2 1.12789823445\n",
      "dW norm  633.201445325\n",
      " dW decay L1  633.201285393\n",
      " dW mom  1033.05866681\n",
      " h_bias  0.00214411580755\n",
      " v_bias  195.129628914\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.249718898989 0.00214411580755 195.129628914\n",
      "batch norm  87.2872290323\n",
      "init batch:  87.2872290323\n",
      "mean hiddens W:  0.249718898989  v:  87.2872290323  h:  0.00214411580755  p:  7.88114142967\n",
      "v, h init norm post init 87.2872290323 78.2261163467\n",
      "h pos norm  78.2261163467\n",
      "eq  0 84.3991447242 78.2381378724\n",
      "eq  1 83.4065540431 78.2507552511\n",
      "eq  2 83.0254025026 78.2606114564\n",
      "v, h neg norm post eq  83.0254025026 78.2606114564\n",
      "dW naive 715.557758448\n",
      "dW tap2 1.09096529504\n",
      "dW norm  715.770296696\n",
      " dW decay L1  715.770119726\n",
      " dW mom  881.375001584\n",
      " h_bias  0.00216342778507\n",
      " v_bias  195.129844534\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.249690442896 0.00216342778507 195.129844534\n",
      "batch norm  87.1303034633\n",
      "init batch:  87.1303034633\n",
      "mean hiddens W:  0.249690442896  v:  87.1303034633  h:  0.00216342778507  p:  7.74846360811\n",
      "v, h init norm post init 87.1303034633 78.2464101992\n",
      "h pos norm  78.2464101992\n",
      "eq  0 84.3595852315 78.2741330804\n",
      "eq  1 83.4357386629 78.3024144056\n",
      "eq  2 83.0834757613 78.3239992782\n",
      "v, h neg norm post eq  83.0834757613 78.3239992782\n",
      "dW naive 621.343776196\n",
      "dW tap2 1.07402979299\n",
      "dW norm  621.662604162\n",
      " dW decay L1  621.662127212\n",
      " dW mom  823.91855788\n",
      " h_bias  0.00220412415284\n",
      " v_bias  195.129842869\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.249081031433 0.00220412415284 195.129842869\n",
      "batch norm  82.4626679104\n",
      "init batch:  82.4626679104\n",
      "mean hiddens W:  0.249081031433  v:  82.4626679104  h:  0.00220412415284  p:  6.21641518388\n",
      "v, h init norm post init 82.4626679104 78.5913316319\n",
      "h pos norm  78.5913316319\n",
      "eq  0 81.9965878741 78.5765980157\n",
      "eq  1 82.310422659 78.5621981276\n",
      "eq  2 82.5962839847 78.5516119323\n",
      "v, h neg norm post eq  82.5962839847 78.5516119323\n",
      "dW naive 716.850443144\n",
      "dW tap2 1.0404222878\n",
      "dW norm  716.820988998\n",
      " dW decay L1  716.821213478\n",
      " dW mom  733.496070808\n",
      " h_bias  0.00218547549432\n",
      " v_bias  195.129977209\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.256304749189 0.00218547549432 195.129977209\n",
      "batch norm  88.1384466267\n",
      "init batch:  88.1384466267\n",
      "mean hiddens W:  0.256304749189  v:  88.1384466267  h:  0.00218547549432  p:  8.06134786622\n",
      "v, h init norm post init 88.1384466267 78.1852793407\n",
      "h pos norm  78.1852793407\n",
      "eq  0 84.8473475252 78.2374661089\n",
      "eq  1 83.6970926023 78.2903014039\n",
      "eq  2 83.2405491244 78.3303679267\n",
      "v, h neg norm post eq  83.2405491244 78.3303679267\n",
      "dW naive 650.916345938\n",
      "dW tap2 1.05464756998\n",
      "dW norm  651.265344432\n",
      " dW decay L1  651.264485335\n",
      " dW mom  670.691892721\n",
      " h_bias  0.00226017712651\n",
      " v_bias  195.12985321\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.252427476084 0.00226017712651 195.12985321\n",
      "batch norm  84.1294107578\n",
      "init batch:  84.1294107578\n",
      "mean hiddens W:  0.252427476084  v:  84.1294107578  h:  0.00226017712651  p:  6.67371628746\n",
      "v, h init norm post init 84.1294107578 78.5490943777\n",
      "h pos norm  78.5490943777\n",
      "eq  0 82.7997209755 78.5376734296\n",
      "eq  1 82.7034441806 78.5267868919\n",
      "eq  2 82.7882374584 78.51896775\n",
      "v, h neg norm post eq  82.7882374584 78.51896775\n",
      "dW naive 881.648097064\n",
      "dW tap2 1.02398792236\n",
      "dW norm  881.614998335\n",
      " dW decay L1  881.615141933\n",
      " dW mom  951.992643164\n",
      " h_bias  0.00224702118131\n",
      " v_bias  195.129951668\n",
      "(100, 784)\n",
      "W, hb vb norm start batch 0.258509970523 0.00224702118131 195.129951668\n",
      "batch norm  84.7297529443\n",
      "init batch:  84.7297529443\n",
      "mean hiddens W:  0.258509970523  v:  84.7297529443  h:  0.00224702118131  p:  8.0261461543\n",
      "v, h init norm post init 84.7297529443 78.333934403\n",
      "h pos norm  78.333934403\n",
      "eq  0 83.1629056661 78.3626151689\n",
      "eq  1 82.8797724148 78.3923185326\n",
      "eq  2 82.8569849184 78.4152721233\n",
      "v, h neg norm post eq  82.8569849184 78.4152721233\n",
      "dW naive 556.982192725\n",
      "dW tap2 1.05712825771\n",
      "dW norm  557.15974417\n",
      " dW decay L1  557.159199369\n",
      " dW mom  758.521682442\n",
      " h_bias  0.00229120308147\n",
      " v_bias  195.129821485\n",
      "(100, 784)\n",
      "W, hb vb norm start batch"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-4566c16bf014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-cf7fade428ed>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(rbm, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_slices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\" . \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-f320b3802c84>\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(rbm, X_batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"W, hb vb norm start batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"batch norm \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Duplicate axes given.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0m_multi_svd_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multi_svd_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36m_multi_svd_norm\u001b[0;34m(x, row_axis, col_axis, op)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0mrow_axis\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->d'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(rbm, X[0:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:32:47.682919",
     "start_time": "2016-09-30T22:32:47.340Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try classifier\n",
    "\n",
    "#### should we be using the EMF estimator?\n",
    "\n",
    "what are the correlations...do they drop to 0 as we converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:34.455630",
     "start_time": "2016-09-05T00:26:34.452408"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.158747",
     "start_time": "2016-09-05T00:26:34.619915"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = sig_means(X, rbm.h_bias , rbm.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.165119",
     "start_time": "2016-09-05T00:26:35.160605"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print p.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.234387",
     "start_time": "2016-09-05T00:26:35.166771"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(p, Y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:27:12.888431",
     "start_time": "2016-09-05T00:26:35.236960"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in [5000]:\n",
    "    lr  = linear_model.LogisticRegression()\n",
    "    lr.C = c\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_test_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "    print c, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:27:12.893304",
     "start_time": "2016-09-05T00:27:12.890460"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note bad, but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
