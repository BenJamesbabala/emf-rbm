{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMF RBM Class Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    " - careful unit tests require removing the regularization, the stochastic inputs, and the denoising\n",
    "     - this has been painful and is not finished yet\n",
    "     \n",
    " - redo init_weights, in partial_fit...or keep ?\n",
    " - write emf_rbm.py  \n",
    " - fix divide_by_zero in means_hiddens\n",
    " - add test between instead of almost\n",
    " \n",
    " - check mnist\n",
    " - do epochs diverge\n",
    "  - how does julia code,BernoulliRBM behave\n",
    " \n",
    " - second derivative!!!\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T18:57:51.977663",
     "start_time": "2016-10-06T18:57:51.971338"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-01T21:30:48.445229",
     "start_time": "2016-10-01T21:30:48.440001"
    }
   },
   "source": [
    "### use julia data set\n",
    "\n",
    "I don't know how to reproduce their normalization yet\n",
    "\n",
    "TODO: write after it is debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T00:40:26.698114",
     "start_time": "2016-10-07T00:40:25.203089"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile emf_rbm.py\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from sklearn.utils.fixes import expit  # logistic function  \n",
    "from sklearn.utils.extmath import safe_sparse_dot, log_logistic, softmax\n",
    "\n",
    "class EMF_RBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extended Mean Field Restricted Boltzmann Machine (RBM).\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hidden units. Parameters are estimated using the Extended Mean\n",
    "    Field model, based on the TAP equations\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "    momentum : float, optional\n",
    "        gradient momentum parameter\n",
    "    decay : float, optional\n",
    "        decay for weight update regularizer\n",
    "    weight_decay: string, optional []'L1', 'L2', None]\n",
    "        weight update regularizer\n",
    "\n",
    "    neq_steps: int, optional\n",
    "        Number of equilibration steps\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "    sigma: float, optional\n",
    "        variance of initial W weight matrix\n",
    "    thresh: float, optional\n",
    "        threshold for values in W weight matrix, vectors\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    h_bias : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "    v_bias : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "    W : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = EMF_RBM(n_components=2)\n",
    "    >>> model.fit(X)\n",
    "    EmfRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "    References\n",
    "    ----------\n",
    "    [1] Marylou GabrieÂ´, Eric W. Tramel1 and Florent Krzakala1, \n",
    "        Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy\n",
    "        https://arxiv.org/pdf/1506.02914\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100, sigma=0.001, neq_steps = 3,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1', thresh=1e-8):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.neq_steps = neq_steps\n",
    "\n",
    "        # learning rate / mini_batch\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # threshold for floats\n",
    "        self.thresh = thresh\n",
    "\n",
    "        # store in case we want to reset\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "        # h bias\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        # moved to fit\n",
    "        \n",
    "        self.W = None\n",
    "        self.dW_prev = None\n",
    "        self.W2 = None\n",
    "        self.v_bias = None\n",
    "        \n",
    "\n",
    "    def init_weights(self, X):\n",
    "        \"\"\" If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "        initialize the visibile biases according to the empirical expected                                                                                \n",
    "        feature values of the training data.                                                                                                              \n",
    "\n",
    "        TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "               the case of binary RBMs.\n",
    "        \"\"\"\n",
    "        # \n",
    "        eps = self.thresh\n",
    "\n",
    "        # Mean across  samples \n",
    "        if issparse(X):\n",
    "            probVis = csr_matrix.mean(X, axis=0)\n",
    "        else:\n",
    "            probVis = np.mean(X,axis=0)            \n",
    "\n",
    "        # safe for CSR / sparse mats ?\n",
    "        # do we need it if we use softmax ?\n",
    "        probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "        #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "        self.v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion\n",
    "        \n",
    "        # (does not work)\n",
    "        # self.v_bias = softmax(probVis)\n",
    "        \n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.random_state.normal(\n",
    "                0,\n",
    "                self.sigma,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "\n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def sample_layer(self, layer):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v) or P(v|h)\"\"\"\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        sample = (self.random_state.random_sample(size=layer.shape) < layer) \n",
    "        return sample\n",
    "\n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        return self.sample_layer(self._mean_hiddens(v))\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the conditional probabilities P(h=1|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        return sample_layer(self._mean_visible(h))\n",
    "\n",
    "    def _mean_visibles(self, h):\n",
    "        \"\"\"Computes the conditional probabilities P(v=1|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        Returns\n",
    "        -------\n",
    "         v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.     \n",
    "        \"\"\"\n",
    "        #p = np.dot(h, self.W) + self.v_bias\n",
    "        p = safe_sparse_dot(h, W) + self.v_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def sigma_means(self, x, b, W):\n",
    "        \"\"\"helper class for computing Wx+b \"\"\"\n",
    "        a = safe_sparse_dot(x, W.T) + b\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def init_batch(self, vis):\n",
    "        \"\"\"initialize the batch for EMF only\"\"\"\n",
    "        v_pos = vis\n",
    "        v_init = v_pos\n",
    "\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        h_init = h_pos\n",
    "\n",
    "        return v_pos, h_pos, v_init, h_init\n",
    "\n",
    "    def equilibrate(self, v0, h0, iters=3):\n",
    "        \"\"\"Run iters steps of the TAP fixed point equations\"\"\"\n",
    "        mv = v0\n",
    "        mh = h0\n",
    "     \n",
    "        for i in range(iters):\n",
    "            mv = 0.5 *self.mv_update(mv, mh) + 0.5*mv\n",
    "            mh = 0.5 *self.mh_update(mv, mh) + 0.5*mh\n",
    "            print \"mv, mh\", i, np.linalg.norm(mv, ord=2), np.linalg.norm(mh, ord=2)\n",
    "        return mv, mh\n",
    "\n",
    "    def mv_update(self, v, h):  \n",
    "        \"\"\"update TAP visbile magnetizations, to second order\"\"\"\n",
    "        \n",
    "        # a = np.dot(h, self.W) + self.v_bias\n",
    "        a = safe_sparse_dot(h, self.W) + self.v_bias\n",
    "\n",
    "        h_fluc = h-(h*h)\n",
    "        a += h_fluc.dot(self.W2)*(0.5-v)\n",
    "        #a += safe_sparse_dot(h_fluc,self.W2)*(0.5-v)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def mh_update(self, v, h):\n",
    "        \"\"\"update TAP hidden magnetizations, to second order\"\"\"\n",
    "        a = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "\n",
    "        v_fluc = (v-(v*v))\n",
    "        a += v_fluc.dot((self.W2).T)*(0.5-h)\n",
    "        #a += safe_sparse_dot(v_fluc,self.W2.T)*(0.5-h)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "\n",
    "    def weight_gradient(self, v_pos, h_pos ,v_neg, h_neg):\n",
    "        \"\"\"compute weight gradient of the TAP Free Energy, to second order\"\"\"\n",
    "        # naive  / mean field\n",
    "        dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "\n",
    "        print \"dW naive\", np.linalg.norm(dW,ord=2)\n",
    "        \n",
    "        # tap2 correction\n",
    "        h_fluc = (h_neg - (h_neg*h_neg)).T\n",
    "        v_fluc = (v_neg - (v_neg*v_neg))\n",
    "        #  dW_tap2 = h_fluc.dot(v_fluc)*self.W\n",
    "        dW_tap2 = safe_sparse_dot(h_fluc,v_fluc)*self.W\n",
    "\n",
    "        print \"dW_tap2\", np.linalg.norm(dW_tap2,ord=2)\n",
    "\n",
    "        dW -= dW_tap2\n",
    "        return dW\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "        Onsager = -0.5*(dW_tap2).sum(axis=1)\n",
    "\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "\n",
    "        return fe_tap - fe\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def score_samples_TAP(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X using second order TAP\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')      \n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy_TAP(v)\n",
    "        fe_ = self._free_energy_TAP(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    def _corrupt_data(self, v):\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \"\"\"Randomly corrupt one feature in each sample in v.\"\"\"\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               self.random_state.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "        return v, v_\n",
    "    \n",
    "    \n",
    "    def score_samples_entropy(self, X):\n",
    "        \"\"\"Compute the entropy of X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            Value of the entropy.\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the entropy on X,\n",
    "        then on a randomly corrupted version of X, and returns the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        s = self._entropy(v)\n",
    "        s_ = self._entropy(v_)\n",
    "        return v.shape[1] * (s_ - s)\n",
    "\n",
    "    \n",
    "    #TODO: fix later\n",
    "    def _denoise(m, eps=1e-8):\n",
    "        \"\"\"denoise magnetization\"\"\"\n",
    "      #  m[m < eps] = eps\n",
    "        return m\n",
    "\n",
    "\n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "        Onsager = -0.5*(dW_tap2).sum(axis=1)\n",
    "\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "\n",
    "        return fe_tap - fe\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "    \n",
    "    \n",
    "    def _entropy(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            The value of the entropy.\n",
    "        \"\"\"\n",
    "         \n",
    "        h = self._mean_hiddens(v)\n",
    "        print \"entropy v, h \",np.linalg.norm(v, ord=2),np.linalg.norm(h, ord=2)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        print \"entropy mv, mh \",np.linalg.norm(mv, ord=2),np.linalg.norm(mh, ord=2)\n",
    "\n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # appears to be wrong ?  unsure why ?  maybe because it is not denoised !!!\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)) \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)) ).sum(axis=1) \n",
    "        \n",
    "        print Entropy[0:5]\n",
    "                         \n",
    "        return Entropy\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        ## remove this \n",
    "        print \"partial fit X norm\", np.linalg.norm(X, ord=2)\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        print \"partial fit X norm\", np.linalg.norm(X, ord=2)\n",
    "\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.W = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'h_bias'):\n",
    "            self.h_bias = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'v_bias'):\n",
    "            self.v_bias = np.zeros(X.shape[1], )\n",
    "\n",
    "        # not used ?\n",
    "        #if not hasattr(self, 'h_samples_'):\n",
    "        #    self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        print \"partial fit X norm\", np.linalg.norm(X, ord=2)\n",
    "        self._fit(X)\n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Extended Mean Field theory (second order TAP equations).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "        \"\"\"\n",
    "        X_batch = v_pos\n",
    "        print \"X_batch norm \",np.linalg.norm(X_batch, ord=2)\n",
    "        lr = float(self.learning_rate) / X_batch.shape[0]\n",
    "        decay = self.decay\n",
    "\n",
    "        v_pos, h_pos, v_init, h_init = self.init_batch(X_batch)\n",
    "      \n",
    "        print \"v_init, h_init \",np.linalg.norm(v_init, ord=2),np.linalg.norm(h_init, ord=2) \n",
    "        \n",
    "        a = safe_sparse_dot(h_init, self.W) + self.v_bias\n",
    "        a = expit(a, out=a)\n",
    "\n",
    "        # get_negative_samples\n",
    "        v_neg, h_neg = self.equilibrate(v_init, h_init, iters=self.neq_steps) \n",
    "        \n",
    "        # basic gradient\n",
    "        dW = self.weight_gradient(v_pos, h_pos ,v_neg, h_neg) \n",
    "        print \"dW grad \", np.linalg.norm(dW, ord=2)\n",
    "\n",
    "        # regularization based on weight decay\n",
    "        #  similar to momentum >\n",
    "        if self.weight_decay == \"L1\":\n",
    "            print \" decay \",self.decay , np.linalg.norm(np.sign(self.W), ord=2), np.linalg.norm(self.W, ord=2)\n",
    "            dW -= decay * np.sign(self.W)\n",
    "        elif self.weight_decay == \"L2\":\n",
    "            dW -= decay * self.W\n",
    "            \n",
    "        print \"dW decay \",self.weight_decay , np.linalg.norm(dW, ord=2)\n",
    "\n",
    "        # can we use BLAS here ?\n",
    "        # momentum\n",
    "        # note:  what do we do if lr changes per step ? not ready yet\n",
    "        dW += self.momentum * self.dW_prev  \n",
    "        print \"dW mom\", np.linalg.norm(dW, ord=2)\n",
    "        # update\n",
    "        self.W += lr * dW \n",
    "        print \"|W|\", np.linalg.norm(self.W, ord=2)\n",
    "\n",
    "\n",
    "        # storage for next iteration\n",
    "\n",
    "        # is this is a memory killer \n",
    "        self.dW_prev =  dW  \n",
    "        \n",
    "        # is this wasteful...can we avoid storing 2X the W mat ?\n",
    "        self.W2 = self.W*self.W\n",
    "\n",
    "        # update bias terms\n",
    "        self.h_bias += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0))\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        verbose = self.verbose\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        self.init_weights(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        \n",
    "\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        \n",
    "        begin = time.time()\n",
    "        for iteration in xrange(1, self.n_iter + 1):\n",
    "            #print \"iter \", iteration\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "\n",
    "            #print \"batches done\"\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:52:34.845155",
     "start_time": "2016-10-03T20:52:34.841587"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:35:48.241019",
     "start_time": "2016-10-03T20:35:48.236692"
    }
   },
   "source": [
    "# EMF Tests\n",
    "\n",
    "Designed to match results of julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:32:59.404629",
     "start_time": "2016-10-03T20:32:59.402070"
    }
   },
   "source": [
    "## Test EMF class init\n",
    "\n",
    "Xdigits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T00:40:26.814191",
     "start_time": "2016-10-07T00:40:26.699891"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "from scipy.sparse import csc_matrix, csr_matrix, lil_matrix\n",
    "from sklearn.utils.testing import (assert_almost_equal, assert_array_equal,\n",
    "                                   assert_true)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "np.seterr(all='warn')\n",
    "\n",
    "Xdigits = load_digits().data\n",
    "Xdigits -= Xdigits.min()\n",
    "Xdigits /= Xdigits.max()\n",
    "\n",
    "b = Binarizer(threshold=0.001, copy=True)\n",
    "Xdigits = b.fit_transform(Xdigits)\n",
    "print Xdigits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset for julia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:43.534724",
     "start_time": "2016-10-03T21:08:43.529474"
    }
   },
   "source": [
    "X = Xdigits.copy()\n",
    "hf =  h5py.File('xdigits.h5',\"w\")\n",
    "hf.create_dataset(\"X\", data=X) \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:50.521176",
     "start_time": "2016-10-03T21:08:50.513828"
    }
   },
   "source": [
    "hf =  h5py.File('xdigits.h5',\"r\")\n",
    "print(\"keys\",hf.keys())\n",
    "X = np.array(hf.get('X'))\n",
    "hf.close()\n",
    "print \"norm of X \",np.linalg.norm(X,ord=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### julia  xdigits_ex.jl   results checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:37:07.818848",
     "start_time": "2016-10-03T21:37:07.813802"
    }
   },
   "source": [
    "### test init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T00:40:27.390464",
     "start_time": "2016-10-07T00:40:27.368078"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    X = Xdigits.copy()\n",
    "    assert_almost_equal(np.linalg.norm(X,ord=2), 211.4983270228649  , decimal=12)\n",
    "\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=50 , decay=0.01, learning_rate=0.005, n_iter=0, sigma=0.001, neq_steps=3, verbose=True)\n",
    "    rbm.fit(X)\n",
    "    assert_true(np.linalg.norm(rbm.h_bias, ord=2)==0.0)\n",
    "    assert_true(np.linalg.norm(rbm.lr)==0.005)\n",
    "    assert_true(np.linalg.norm(rbm.momentum)==0.5)\n",
    "    assert_true(np.linalg.norm(rbm.decay)==0.01)\n",
    "    assert_true(np.linalg.norm(rbm.n_iter)==0)\n",
    "    assert_true(np.linalg.norm(rbm.neq_steps)==3)\n",
    "    assert_true(np.linalg.norm(rbm.sigma)==0.001)\n",
    "    assert_true(np.linalg.norm(rbm.verbose)==True)\n",
    "    assert_true(np.linalg.norm(rbm.n_components)==64)\n",
    "    assert_true(np.linalg.norm(rbm.thresh)==1e-8)\n",
    "    assert_true(np.linalg.norm(rbm.batch_size)==50)\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.97455, decimal=5)\n",
    "\n",
    "    #assert_true(np.linalg.norm(rbm.weight_decay)=='L1')\n",
    "    assert_array_equal(X, Xdigits)\n",
    "    \n",
    "test_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one batch, $\\sigma\\sim0$, decay = 0 (no regularization)\n",
    "\n",
    "the values are slightly off...why  ?\n",
    "\n",
    "I think it is the sign of the regularizer --  even when |W| is small, it is reset to 1  \n",
    "that seems wrong...but\n",
    "\n",
    "norm of W  0.007628825568441182  \n",
    "hb 1.004859173557616e-18  \n",
    "vb 38.97452180357592\n",
    "\n",
    "prev dW updated 152.57651136882203\n",
    "\n",
    "pseudo l-hood: -24.141607789155778  \n",
    "<font color='red'> entropy and energies include denoising, which changes values slightly</font>\n",
    "\n",
    "\n",
    "entropy: 68.52763153155018  \n",
    "TAP free_energy: -24.721559208225475  \n",
    "U naive: -48.56691662443823  \n",
    "free energy: -9268.746331749979"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T00:40:28.152554",
     "start_time": "2016-10-07T00:40:28.081651"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(100, 64) 49.3103298921\n",
      "partial fit X norm 49.3103298921\n",
      "partial fit X norm 49.3103298921\n",
      "partial fit X norm 49.3103298921\n",
      "X_batch norm  49.3103298921\n",
      "v_init, h_init  49.3103298921 40.0\n",
      "mv, mh 0 49.4387814515 40.0\n",
      "mv, mh 1 49.5605512502 40.0\n",
      "mv, mh 2 49.6362030547 40.0\n",
      "dW naive 152.576511369\n",
      "dW_tap2 7.03869231325e-15\n",
      "dW grad  152.576511369\n",
      "dW decay  None 152.576511369\n",
      "dW mom 152.576511369\n",
      "|W| 0.00762882556844\n",
      "mv, mh 0 49.434945266 39.9904674155\n",
      "mv, mh 1 49.5538660863 39.9884164661\n",
      "mv, mh 2 49.6278564858 39.9868785004\n",
      "entropy v, h  49.3103298921 39.9925187414\n",
      "mv, mh 0 49.434945266 39.9904674155\n",
      "mv, mh 1 49.5538660863 39.9884164661\n",
      "mv, mh 2 49.6278564858 39.9868785004\n",
      "entropy mv, mh  49.6278564858 39.9868785004\n",
      "[ 68.79461161  67.89909489  68.12303767  67.81475161  68.82954536]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 7 decimals\n ACTUAL: 68.527587598318064\n DESIRED: 68.52763153155018",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-74085f7ceec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtest_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-137-74085f7ceec1>\u001b[0m in \u001b[0;36mtest_one_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# why is this wrong ?  unsure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_entropy\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m68.52763153155018\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m#assert_almost_equal(avg_free_energy_tap, -24.721559208225475)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/numpy/testing/utils.pyc\u001b[0m in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_build_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 7 decimals\n ACTUAL: 68.527587598318064\n DESIRED: 68.52763153155018"
     ]
    }
   ],
   "source": [
    "def test_one_batch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.00, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=1e-16, neq_steps=3, verbose=True, weight_decay=None)\n",
    "    rbm.init_weights(X);\n",
    "      \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2), 0.0)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W2,ord=2), 0.0 )\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),0.0 )\n",
    "    \n",
    "    print X.shape\n",
    "    X_batch = X[0:100,:]\n",
    "    print X_batch.shape, np.linalg.norm(X_batch,ord=2)\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.31032989212045)\n",
    "\n",
    "    rbm.partial_fit(X_batch);\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2), 0.007628825568441182 )\n",
    "    \n",
    "    scored_free_energy = np.average(rbm.score_samples(X_batch))\n",
    "    avg_free_energy_tap = np.average(rbm._free_energy_TAP(X_batch))\n",
    "    avg_entropy = np.average(np.average(rbm._entropy(X_batch)))\n",
    "        \n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.9745218036)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0 )\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2), 152.57651136882203)\n",
    "\n",
    "    # why is this wrong ?  unsure\n",
    "    assert_almost_equal(avg_entropy,  68.52763153155018 )\n",
    "    #assert_almost_equal(avg_free_energy_tap, -24.721559208225475)\n",
    "\n",
    "    print scored_free_energy, avg_free_energy_tap, avg_entropy\n",
    "    return rbm\n",
    "\n",
    "test_one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one epoch, $\\sigma=0.001$\n",
    "\n",
    "compare:\n",
    "\n",
    "5 julia runs  \n",
    "batch norm of W, hb, vb  0.015177951725370209 6.125160958113443e-5 38.974531344645186  \n",
    "batch norm of W, hb, vb  0.016005072745766846 6.132506125735679e-5 38.974534343561935  \n",
    "batch norm of W, hb, vb  0.015518275427920199 6.143705375221393e-5 38.97453267232916\n",
    "batch norm of W, hb, vb  0.016618832753491925 6.14604623830071e-5 38.97453303623846\n",
    "batch norm of W, hb, vb  0.015643733669880935 6.131198883353152e-5 38.97453109464897\n",
    "\n",
    "10 BernoulliRBM runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T18:14:15.163516",
     "start_time": "2016-10-06T18:14:15.161215"
    }
   },
   "source": [
    "# <font color='red'>fix above</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T18:03:21.873343",
     "start_time": "2016-10-06T18:03:21.747957"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_one_epoch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=1, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974531, decimal=4)\n",
    "    # really between 0.015 and 0.0165: hard to test properly with a single statement\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0165, decimal=2)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000061, decimal=2)\n",
    "    \n",
    "    # non tap FE totally wrong\n",
    "    # FE ~ -2x.x\n",
    "\n",
    "    scored_free_energy = np.average(rbm.score_samples(X))\n",
    "    \n",
    "    avg_free_energy_tap = np.average(rbm._free_energy_TAP(X))\n",
    "    avg_entropy = np.average(np.average(rbm._entropy(X)))\n",
    "    \n",
    "    #assert_almost_equal(scored_free_energy, -24, decimal=0)  \n",
    "    #assert_almost_equal(avg_free_energy_tap, -25, decimal=0)  \n",
    "    assert_almost_equal(avg_entropy, 68.8, decimal=0)\n",
    "    return rbm\n",
    "\n",
    "for i in range(1):\n",
    "    test_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:09:44.721707",
     "start_time": "2016-10-06T17:09:44.719076"
    }
   },
   "source": [
    "###  Exact Result, $\\sigma\\sim0$\n",
    "\n",
    "julia result:\n",
    "\n",
    "TAP free_energy: -23.272298000408423\n",
    "entropy: 68.80871848695904    \n",
    "\n",
    "updated h_bias 6.073000902126526e-5  \n",
    "updated v_bias 38.97454137433605\n",
    "\n",
    "### <font color='red'>we are off by 0.1 ?  why </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T18:03:23.612653",
     "start_time": "2016-10-06T18:03:22.724697"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_one_epoch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=1, \n",
    "                  sigma=1e-16, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "\n",
    "    scored_free_energy = np.average(rbm.score_samples(X))\n",
    "    \n",
    "    avg_free_energy_tap = np.average(rbm._free_energy_TAP(X))\n",
    "    avg_entropy = np.average(np.average(rbm._entropy(X)))\n",
    "\n",
    "    print scored_free_energy, avg_free_energy_tap, avg_entropy, np.linalg.norm(rbm.v_bias, ord=2), np.linalg.norm(rbm.h_bias, ord=2)\n",
    "    return rbm\n",
    "\n",
    "for i in range(10):\n",
    "    test_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T22:07:51.818339",
     "start_time": "2016-10-03T22:07:51.814845"
    }
   },
   "source": [
    "### test partial fit  (1 iter, 1 batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:54.937747",
     "start_time": "2016-10-06T15:09:54.400217"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.000000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.9745518)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W2,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2), 0.000000001)\n",
    "\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.007629, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974521, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0, decimal=3)    \n",
    "    \n",
    "    #there are large variations in dw_prev\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),152.6, decimal=1)\n",
    "\n",
    "# test stochastically (sometimes will fail due to roundoff error in dw_prev)\n",
    "for i in range(100):\n",
    "    test_partial_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T16:04:44.744505",
     "start_time": "2016-10-04T16:04:44.742373"
    }
   },
   "source": [
    "### Test 2 iterations of the partial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:56.380427",
     "start_time": "2016-10-06T15:09:55.808980"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit_2iters():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.00000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    \n",
    "    X_batch = Xdigits.copy()[100:200]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 48.96867960939811)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974504602)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000001, decimal=6)\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0154, decimal=3)\n",
    "    # not correct ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),177.75, decimal=1)\n",
    "   \n",
    "\n",
    "# test stochastically\n",
    "for i in range(100):\n",
    "    test_partial_fit_2iters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:09:59.555197",
     "start_time": "2016-10-06T15:09:57.145138"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "    \n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"1 epoch [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\"\n",
    "\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "\n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"20 epochs [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So |W| can vary uptp %5, even after 20 epochs  \n",
    "and the variation across runs >> variation across epochs\n",
    "\n",
    "which make it difficult to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fit 20 epochs on xdigits\n",
    "\n",
    "### <font color='red'>energies are not deterministic .  need to remove noise and check </font>\n",
    "\n",
    "## TODO; check log likelihood as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:19:44.045186",
     "start_time": "2016-10-06T15:19:43.420883"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_fit_xdigits():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=20, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    \n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.02, decimal=1)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.9747, decimal=3)\n",
    "    # why is h so different ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0012, decimal=2)\n",
    "    return rbm\n",
    "    \n",
    "rbm = test_fit_xdigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the varation in the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.317590",
     "start_time": "2016-10-06T15:19:44.046713"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_s, vb_s, hb_s = [], [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rbm = test_fit_xdigits();\n",
    "    w_s.append(np.linalg.norm(rbm.W, ord=2))\n",
    "    vb_s.append(np.linalg.norm(rbm.v_bias, ord=2))\n",
    "    hb_s.append(np.linalg.norm(rbm.h_bias, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.638613",
     "start_time": "2016-10-06T15:27:40.319354"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(w_s,50);\n",
    "plt.title(\"distribution of |W|\")\n",
    "print \"mean |W| \", np.mean(np.array(w_s))\n",
    "print \"std |W| \", np.std(np.array(w_s)), 100.0*np.std(np.array(w_s))/np.mean(np.array(w_s)), \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:40.974751",
     "start_time": "2016-10-06T15:27:40.640289"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(vb_s,50);\n",
    "plt.title(\"distribution of |v_bias|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.270756",
     "start_time": "2016-10-06T15:27:40.976424"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(hb_s,50);\n",
    "plt.title(\"distribution of |h_bias|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test sample hiddens\n",
    "\n",
    "IDK why there is a divide-by-zero error\n",
    "\n",
    "Perhaps the exitp() needs some regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.297486",
     "start_time": "2016-10-06T15:27:41.272250"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sample_hiddens():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = Xdigits[:100]\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    h = rbm1._mean_hiddens(X[0])\n",
    "    hs = np.mean([rbm1._sample_hiddens(X[0]) for i in range(100)], 0)\n",
    "\n",
    "    assert_almost_equal(h, hs, decimal=1)\n",
    "\n",
    "test_sample_hiddens()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>why divide by zero error ? <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.513190",
     "start_time": "2016-10-06T15:27:41.299490"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six.moves import cStringIO as StringIO\n",
    "def test_rbm_verbose():\n",
    "    rbm = EMF_RBM(n_iter=2, verbose=10)\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "    try:\n",
    "        rbm.fit(Xdigits)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "test_rbm_verbose()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:29:02.736552",
     "start_time": "2016-10-04T01:29:02.734464"
    }
   },
   "source": [
    "### Test Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.548651",
     "start_time": "2016-10-06T15:27:41.514911"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_transform():\n",
    "    X = Xdigits[:110] # using 100 causes divide by zero error in mean_hiddens()!\n",
    "    rbm1 = EMF_RBM(n_components=16, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    Xt1 = rbm1.transform(X)\n",
    "    Xt2 = rbm1._mean_hiddens(X)\n",
    "\n",
    "    assert_array_equal(Xt1, Xt2)\n",
    "    \n",
    "test_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test means_hidden\n",
    "\n",
    "should compare to older RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.562481",
     "start_time": "2016-10-06T15:27:41.550071"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.array([[0.], [1.]])\n",
    "rbm = BernoulliRBM(n_components=2, batch_size=2,\n",
    "                    n_iter=42, random_state=rng)\n",
    "# you need that much iters\n",
    "rbm.fit(X)\n",
    "#assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "#assert_almost_equal(rbm1.gibbs(X), X)\n",
    "print rbm.components_\n",
    "print rbm.intercept_hidden_\n",
    "print rbm.intercept_visible_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:42:12.136222",
     "start_time": "2016-10-05T23:42:12.133731"
    }
   },
   "source": [
    "### <font color='red'>why is this off ? </font>\n",
    "\n",
    "Could this be due to the regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T15:27:41.586972",
     "start_time": "2016-10-06T15:27:41.564416"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_mean_hiddens():\n",
    "    # Im not entirely sure why this happens, but the hidden units all go to 1/2\n",
    "    # and the h array is (2,2)\n",
    "    # h never changes ... WTf ?!\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=42, random_state=rng, \n",
    "                        decay = 0.0, weight_decay=None, momentum=0)\n",
    "    rbm.fit(X)\n",
    "    h = rbm._mean_hiddens(X)\n",
    "    assert_true(h.shape==(2,2))\n",
    "    assert_almost_equal(np.linalg.norm(h,ord=2), 1.0, decimal=4)\n",
    "    assert_almost_equal(h[0,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[0,1], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,1], 0.5, decimal=3)\n",
    "\n",
    "    print h\n",
    "    print rbm.W\n",
    "    print rbm.v_bias\n",
    "    \n",
    "test_mean_hiddens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test equlibrate\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:01:14.557937",
     "start_time": "2016-10-06T17:01:14.543366"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_fit_equilibrate():\n",
    "    # Equlibrate on the RBM hidden layer should be able to recreate [[0], [1]]\n",
    "    # from the same input\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=42, random_state=rng)\n",
    "    # you need that much iters\n",
    "    rbm1.fit(X)\n",
    "    #assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "    #assert_almost_equal(rbm1.gibbs(X), X)\n",
    "    return rbm1, X\n",
    "\n",
    "rbm, X = test_fit_equilibrate()\n",
    "print rbm.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using sparse CSR matrix\n",
    "\n",
    "### <font color='red'>must implement sparse matrices: not so simple <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-06T17:01:15.585208",
     "start_time": "2016-10-06T17:01:15.527204"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_small_sparse():\n",
    "    # EMF_RBM should work on small sparse matrices.\n",
    "    X = csr_matrix(Xdigits[:4])\n",
    "    EMF_RBM().fit(X)       # no exception\n",
    "\n",
    "test_small_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more tests TODO\n",
    "\n",
    "#### in sklearn\n",
    "- test_small_sparse():\n",
    "- test_small_sparse_partial_fit():\n",
    "- test_sample_hiddens():\n",
    "- test_equilibrate():\n",
    "- test_equilibrate_sparse():\n",
    "- test_equilibrate_smoke():\n",
    "- test_score_samples():\n",
    "- test_sparse_and_verbose():\n",
    "\n",
    "\n",
    "\n",
    "#### system tests\n",
    "\n",
    "- check that code works with 2 or more iterations\n",
    "- check  mnist, with accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:06:25.316119",
     "start_time": "2016-10-04T01:06:25.298496"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try classifier\n",
    "\n",
    "#### should we be using the EMF estimator?\n",
    "\n",
    "what are the correlations...do they drop to 0 as we converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.498831",
     "start_time": "2016-10-05T21:22:24.636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499224",
     "start_time": "2016-10-05T21:22:24.819Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = sig_means(X, rbm.h_bias , rbm.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499573",
     "start_time": "2016-10-05T21:22:24.986Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print p.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500034",
     "start_time": "2016-10-05T21:22:25.162Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(p, Y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500518",
     "start_time": "2016-10-05T21:22:25.332Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in [5000]:\n",
    "    lr  = linear_model.LogisticRegression()\n",
    "    lr.C = c\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_test_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "    print c, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.501155",
     "start_time": "2016-10-05T21:22:25.515Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note bad, but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
