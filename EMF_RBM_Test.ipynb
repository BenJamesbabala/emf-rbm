{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMF RBM Class Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    " - redo init_weights, in partial_fit...or keep ?\n",
    " - write emf_rbm.py  \n",
    " - fix divide_by_zero in means_hiddens\n",
    " - add test between instead of almost\n",
    " \n",
    " - check mnist\n",
    " - do epochs diverge\n",
    "  - how does julia code,BernoulliRBM behave\n",
    " \n",
    " - second derivative!!!\n",
    " \n",
    " \n",
    " ## TODO; check log likelihood as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:33.392668",
     "start_time": "2016-10-05T23:01:31.368800"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-01T21:30:48.445229",
     "start_time": "2016-10-01T21:30:48.440001"
    }
   },
   "source": [
    "### use julia data set\n",
    "\n",
    "I don't know how to reproduce their normalization yet\n",
    "\n",
    "TODO: write after it is debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:35.757798",
     "start_time": "2016-10-05T23:01:34.748906"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### %load emf_rbm.py\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from sklearn.utils.fixes import expit  # logistic function  \n",
    "from sklearn.utils.extmath import safe_sparse_dot, log_logistic, softmax\n",
    "\n",
    "class EMF_RBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extended Mean Field Restricted Boltzmann Machine (RBM).\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hidden units. Parameters are estimated using the Extended Mean\n",
    "    Field model, based on the TAP equations\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "    momentum : float, optional\n",
    "        gradient momentum parameter\n",
    "    decay : float, optional\n",
    "        decay for weight update regularizer\n",
    "    weight_decay: string, optional []'L1', 'L2', None]\n",
    "        weight update regularizer\n",
    "\n",
    "    neq_steps: int, optional\n",
    "        Number of equilibration steps\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "    sigma: float, optional\n",
    "        variance of initial W weight matrix\n",
    "    thresh: float, optional\n",
    "        threshold for values in W weight matrix, vectors\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    h_bias : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "    v_bias : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "    W : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = EMF_RBM(n_components=2)\n",
    "    >>> model.fit(X)\n",
    "    EmfRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "    References\n",
    "    ----------\n",
    "    [1] Marylou GabrieÂ´, Eric W. Tramel1 and Florent Krzakala1, \n",
    "        Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy\n",
    "        https://arxiv.org/pdf/1506.02914\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100, sigma=0.001, neq_steps = 3,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1', thresh=1e-8):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.neq_steps = neq_steps\n",
    "\n",
    "        # learning rate / mini_batch\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # threshold for floats\n",
    "        self.thresh = thresh\n",
    "\n",
    "        # store in case we want to reset\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "        # h bias\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        # moved to fit\n",
    "        \n",
    "        self.W = None\n",
    "        self.dW_prev = None\n",
    "        self.W2 = None\n",
    "        self.v_bias = None\n",
    "        \n",
    "\n",
    "    def init_weights(self, X):\n",
    "        \"\"\" If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "        initialize the visibile biases according to the empirical expected                                                                                \n",
    "        feature values of the training data.                                                                                                              \n",
    "\n",
    "        TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "               the case of binary RBMs.\n",
    "        \"\"\"\n",
    "        # \n",
    "        eps = self.thresh\n",
    "\n",
    "        # Mean across  samples \n",
    "        if issparse(X):\n",
    "            probVis = csr_matrix.mean(X, axis=0)\n",
    "        else:\n",
    "            probVis = np.mean(X,axis=0)            \n",
    "\n",
    "        # safe for CSR / sparse mats ?\n",
    "        # do we need it if we use softmax ?\n",
    "        probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "        #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "        self.v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion\n",
    "        \n",
    "        # (does not work)\n",
    "        # self.v_bias = softmax(probVis)\n",
    "        \n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.random_state.normal(\n",
    "                0,\n",
    "                self.sigma,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "\n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def sample_layer(self, layer):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v) or P(v|h)\"\"\"\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        sample = (self.random_state.random_sample(size=layer.shape) < layer) \n",
    "        return sample\n",
    "\n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        return self.sample_layer(self._mean_hiddens(v))\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the conditional probabilities P(h=1|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        return sample_layer(self._mean_visible(h))\n",
    "\n",
    "    def _mean_visibles(self, h):\n",
    "        \"\"\"Computes the conditional probabilities P(v=1|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        Returns\n",
    "        -------\n",
    "         v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.     \n",
    "        \"\"\"\n",
    "        #p = np.dot(h, self.W) + self.v_bias\n",
    "        p = safe_sparse_dot(h, W) + self.v_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def sigma_means(self, x, b, W):\n",
    "        \"\"\"helper class for computing Wx+b \"\"\"\n",
    "        a = safe_sparse_dot(x, W.T) + b\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def init_batch(self, vis):\n",
    "        \"\"\"initialize the batch for EMF only\"\"\"\n",
    "        v_pos = vis\n",
    "        v_init = v_pos\n",
    "\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        h_init = h_pos\n",
    "\n",
    "        return v_pos, h_pos, v_init, h_init\n",
    "\n",
    "    def equilibrate(self, v0, h0, iters=3):\n",
    "        \"\"\"Run iters steps of the TAP fixed point equations\"\"\"\n",
    "        mv = v0\n",
    "        mh = h0\n",
    "     \n",
    "        for i in range(iters):\n",
    "            mv = 0.5 *self.mv_update(mv, mh) + 0.5*mv\n",
    "            mh = 0.5 *self.mh_update(mv, mh) + 0.5*mh\n",
    "        return mv, mh\n",
    "\n",
    "    def mv_update(self, v, h):  \n",
    "        \"\"\"update TAP visbile magnetizations, to second order\"\"\"\n",
    "        \n",
    "        # a = np.dot(h, self.W) + self.v_bias\n",
    "        a = safe_sparse_dot(h, self.W) + self.v_bias\n",
    "\n",
    "        h_fluc = h-(h*h)\n",
    "        a += h_fluc.dot(self.W2)*(0.5-v)\n",
    "        #a += safe_sparse_dot(h_fluc,self.W2)*(0.5-v)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def mh_update(self, v, h):\n",
    "        \"\"\"update TAP hidden magnetizations, to second order\"\"\"\n",
    "        a = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "\n",
    "        v_fluc = (v-(v*v))\n",
    "        a += v_fluc.dot((self.W2).T)*(0.5-h)\n",
    "        #a += safe_sparse_dot(v_fluc,self.W2.T)*(0.5-h)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "\n",
    "    def weight_gradient(self, v_pos, h_pos ,v_neg, h_neg):\n",
    "        \"\"\"compute weight gradient of the TAP Free Energy, to second order\"\"\"\n",
    "        # naive  / mean field\n",
    "        dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "\n",
    "        # tap2 correction\n",
    "        h_fluc = (h_neg - (h_neg*h_neg)).T\n",
    "        v_fluc = (v_neg - (v_neg*v_neg))\n",
    "        #  dW_tap2 = h_fluc.dot(v_fluc)*self.W\n",
    "        dW_tap2 = safe_sparse_dot(h_fluc,v_fluc)*self.W\n",
    "\n",
    "        dW -= dW_tap2\n",
    "        return dW\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               self.random_state.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "\n",
    "    \n",
    "    def score_samples_TAP(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X using second order TAP\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               self.random_state.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "\n",
    "        fe = self._free_energy_TAP(v)\n",
    "        fe_ = self._free_energy_TAP(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "\n",
    "    \n",
    "    #TODO: fix later\n",
    "    def _denoise(m, eps=1e-8):\n",
    "        \"\"\"denoise magnetization\"\"\"\n",
    "      #  m[m < eps] = eps\n",
    "        return m\n",
    "\n",
    "\n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "        \n",
    "        #TODO: implement / test\n",
    "        #mv = self._denoise(mv)\n",
    "        #mh = self._denoise(mh)\n",
    "\n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "        Onsager = -0.5*(dW_tap2).sum(axis=1)\n",
    "\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "\n",
    "        return fe_tap - fe\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        ## remove this \n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.W = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'h_bias'):\n",
    "            self.h_bias = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'v_bias'):\n",
    "            self.v_bias = np.zeros(X.shape[1], )\n",
    "\n",
    "        # not used ?\n",
    "        #if not hasattr(self, 'h_samples_'):\n",
    "        #    self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X)\n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Extended Mean Field theory (second order TAP equations).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "        \"\"\"\n",
    "        X_batch = v_pos\n",
    "        \n",
    "        lr = float(self.learning_rate) / X_batch.shape[0]\n",
    "        decay = self.decay\n",
    "\n",
    "        v_pos, h_pos, v_init, h_init = self.init_batch(X_batch)\n",
    "      \n",
    "        a = safe_sparse_dot(h_init, self.W) + self.v_bias\n",
    "        a = expit(a, out=a)\n",
    "\n",
    "        # get_negative_samples\n",
    "        v_neg, h_neg = self.equilibrate(v_init, h_init, iters=self.neq_steps) \n",
    "        \n",
    "        # basic gradient\n",
    "        dW = self.weight_gradient(v_pos, h_pos ,v_neg, h_neg) \n",
    "\n",
    "        # regularization based on weight decay\n",
    "        #  similar to momentum >\n",
    "        if self.weight_decay == \"L1\":\n",
    "            dW -= decay * np.sign(self.W)\n",
    "        elif self.weight_decay == \"L2\":\n",
    "            dW -= decay * self.W\n",
    "\n",
    "        # can we use BLAS here ?\n",
    "        # momentum\n",
    "        # note:  what do we do if lr changes per step ? not ready yet\n",
    "        dW += self.momentum * self.dW_prev  \n",
    "        \n",
    "        # update\n",
    "        self.W += lr * dW \n",
    "\n",
    "        # storage for next iteration\n",
    "\n",
    "        # is this is a memory killer \n",
    "        self.dW_prev =  dW  \n",
    "        \n",
    "        # is this wasteful...can we avoid storing 2X the W mat ?\n",
    "        self.W2 = self.W*self.W\n",
    "\n",
    "        # update bias terms\n",
    "        self.h_bias += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0))\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        verbose = self.verbose\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        self.init_weights(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        \n",
    "\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        \n",
    "        begin = time.time()\n",
    "        for iteration in xrange(1, self.n_iter + 1):\n",
    "            #print \"iter \", iteration\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "\n",
    "            #print \"batches done\"\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:52:34.845155",
     "start_time": "2016-10-03T20:52:34.841587"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:35:48.241019",
     "start_time": "2016-10-03T20:35:48.236692"
    }
   },
   "source": [
    "# EMF Tests\n",
    "\n",
    "Designed to match results of julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T20:32:59.404629",
     "start_time": "2016-10-03T20:32:59.402070"
    }
   },
   "source": [
    "## Test EMF class init\n",
    "\n",
    "Xdigits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:37.347136",
     "start_time": "2016-10-05T23:01:37.174580"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "from scipy.sparse import csc_matrix, csr_matrix, lil_matrix\n",
    "from sklearn.utils.testing import (assert_almost_equal, assert_array_equal,\n",
    "                                   assert_true)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "np.seterr(all='warn')\n",
    "\n",
    "Xdigits = load_digits().data\n",
    "Xdigits -= Xdigits.min()\n",
    "Xdigits /= Xdigits.max()\n",
    "\n",
    "b = Binarizer(threshold=0.001, copy=True)\n",
    "Xdigits = b.fit_transform(Xdigits)\n",
    "print Xdigits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset for julia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:43.534724",
     "start_time": "2016-10-03T21:08:43.529474"
    }
   },
   "source": [
    "X = Xdigits.copy()\n",
    "hf =  h5py.File('xdigits.h5',\"w\")\n",
    "hf.create_dataset(\"X\", data=X) \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:08:50.521176",
     "start_time": "2016-10-03T21:08:50.513828"
    }
   },
   "source": [
    "hf =  h5py.File('xdigits.h5',\"r\")\n",
    "print(\"keys\",hf.keys())\n",
    "X = np.array(hf.get('X'))\n",
    "hf.close()\n",
    "print \"norm of X \",np.linalg.norm(X,ord=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### julia  xdigits_ex.jl   results checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T21:37:07.818848",
     "start_time": "2016-10-03T21:37:07.813802"
    }
   },
   "source": [
    "### test init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:41.396237",
     "start_time": "2016-10-05T23:01:41.320082"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_init():\n",
    "    X = Xdigits.copy()\n",
    "    assert_almost_equal(np.linalg.norm(X,ord=2), 211.4983270228649  , decimal=12)\n",
    "\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=50 , decay=0.01, learning_rate=0.005, n_iter=0, sigma=0.001, neq_steps=3, verbose=True)\n",
    "    rbm.fit(X)\n",
    "    assert_true(np.linalg.norm(rbm.h_bias, ord=2)==0.0)\n",
    "    assert_true(np.linalg.norm(rbm.lr)==0.005)\n",
    "    assert_true(np.linalg.norm(rbm.momentum)==0.5)\n",
    "    assert_true(np.linalg.norm(rbm.decay)==0.01)\n",
    "    assert_true(np.linalg.norm(rbm.n_iter)==0)\n",
    "    assert_true(np.linalg.norm(rbm.neq_steps)==3)\n",
    "    assert_true(np.linalg.norm(rbm.sigma)==0.001)\n",
    "    assert_true(np.linalg.norm(rbm.verbose)==True)\n",
    "    assert_true(np.linalg.norm(rbm.n_components)==64)\n",
    "    assert_true(np.linalg.norm(rbm.thresh)==1e-8)\n",
    "    assert_true(np.linalg.norm(rbm.batch_size)==50)\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.97455, decimal=5)\n",
    "\n",
    "    #assert_true(np.linalg.norm(rbm.weight_decay)=='L1')\n",
    "    assert_array_equal(X, Xdigits)\n",
    "    \n",
    "test_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-03T22:07:51.818339",
     "start_time": "2016-10-03T22:07:51.814845"
    }
   },
   "source": [
    "### test partial fit  (1 iter, 1 batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:43.183105",
     "start_time": "2016-10-05T23:01:42.661727"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.000000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2), 38.9745518)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W2,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2), 0.000000001)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2), 0.000000001)\n",
    "\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.007629, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974521, decimal=4)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0, decimal=3)    \n",
    "    \n",
    "    #there are large variations in dw_prev\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),152.6, decimal=1)\n",
    "\n",
    "# test stochastically (sometimes will fail due to roundoff error in dw_prev)\n",
    "for i in range(100):\n",
    "    test_partial_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T16:04:44.744505",
     "start_time": "2016-10-04T16:04:44.742373"
    }
   },
   "source": [
    "### Test 2 iterations of the partial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:01:44.757152",
     "start_time": "2016-10-05T23:01:44.190713"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_partial_fit_2iters():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=0, \n",
    "                  sigma=0.00000001, neq_steps=3, verbose=True)\n",
    "    rbm.init_weights(X);\n",
    "    X_batch = Xdigits.copy()[0:100]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 49.3103298921)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    \n",
    "    X_batch = Xdigits.copy()[100:200]\n",
    "    assert_almost_equal(np.linalg.norm(X_batch,ord=2), 48.96867960939811)\n",
    "    rbm.partial_fit(X_batch)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974504602)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000001, decimal=6)\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0154, decimal=3)\n",
    "    # not correct ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.dW_prev,ord=2),177.75, decimal=1)\n",
    "   \n",
    "\n",
    "# test stochastically\n",
    "for i in range(100):\n",
    "    test_partial_fit_2iters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one epoch\n",
    "\n",
    "compare:\n",
    "\n",
    "5 julia runs  \n",
    "batch norm of W, hb, vb  0.015177951725370209 6.125160958113443e-5 38.974531344645186  \n",
    "batch norm of W, hb, vb  0.016005072745766846 6.132506125735679e-5 38.974534343561935  \n",
    "batch norm of W, hb, vb  0.015518275427920199 6.143705375221393e-5 38.97453267232916\n",
    "batch norm of W, hb, vb  0.016618832753491925 6.14604623830071e-5 38.97453303623846\n",
    "batch norm of W, hb, vb  0.015643733669880935 6.131198883353152e-5 38.97453109464897\n",
    "\n",
    "10 BernoulliRBM runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:02:25.255196",
     "start_time": "2016-10-05T23:02:22.826039"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch [ 0.917182317896 0.963881900032 ] 0.0466995821354 4.84494854959 %\n",
      "20 epochs [ 0.915879138073 0.96666126054 ] 0.0507821224676 5.25335239349 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "    \n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"1 epoch [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\"\n",
    "\n",
    "w_norms = []\n",
    "for i in range(100):\n",
    "    rbm1 = BernoulliRBM(n_components=64, batch_size=100,\n",
    "                  learning_rate=0.005, n_iter=1,  verbose=False)\n",
    "    X = Xdigits.copy()\n",
    "    rbm1.fit(X);\n",
    "    w_norms.append(np.linalg.norm(rbm1.components_,ord=2))\n",
    "\n",
    "diff= max(w_norms)-min(w_norms)\n",
    "print \"20 epochs [\",min(w_norms),max(w_norms),\"]\", diff, 100.0*diff/max(w_norms),\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So |W| can vary uptp %5, even after 20 epochs  \n",
    "and the variation across runs >> variation across epochs\n",
    "\n",
    "which make it difficult to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:02:49.113155",
     "start_time": "2016-10-05T23:02:37.950996"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_one_epoch():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=1, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.974531, decimal=4)\n",
    "    # really between 0.015 and 0.0165: hard to test properly with a single statement\n",
    "\n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.0165, decimal=2)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.000061, decimal=2)\n",
    "    \n",
    "    # non tap FE totally wrong\n",
    "    # FE ~ -21.x\n",
    "    free_energy = np.average(rbm.score_samples(X))\n",
    "    #print free_energy\n",
    "    #assert_almost_equal(free_energy, -21.0, decimal=0)\n",
    "    \n",
    "    fe_tap = np.average(rbm.score_samples_TAP(X))\n",
    "    #print fe_tap\n",
    "\n",
    "    return rbm\n",
    "\n",
    "for i in range(100):\n",
    "    test_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:02:49.350109",
     "start_time": "2016-10-05T23:02:49.114743"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.8701688707\n",
      "-182.863257773\n"
     ]
    }
   ],
   "source": [
    "rbm = test_one_epoch()\n",
    "rbm.fit(X);\n",
    "print np.average(rbm.score_samples(X))\n",
    "print np.average(rbm.score_samples_TAP(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fit 20 epochs on xdigits\n",
    "\n",
    "# <font color='red'>the TAP free energy is wrong</font>\n",
    "\n",
    "## TODO; check log likelihood as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:03:10.518880",
     "start_time": "2016-10-05T23:03:10.038527"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_fit_xdigits():\n",
    "    X = Xdigits.copy()\n",
    "    rbm = EMF_RBM(momentum=0.5, n_components=64, batch_size=100,\n",
    "                  decay=0.01, learning_rate=0.005, n_iter=20, \n",
    "                  sigma=0.001, neq_steps=3, verbose=False)\n",
    "    rbm.fit(X);\n",
    "    \n",
    "    \n",
    "    \n",
    "    assert_almost_equal(np.linalg.norm(rbm.W,ord=2),0.02, decimal=1)\n",
    "    assert_almost_equal(np.linalg.norm(rbm.v_bias,ord=2),38.9747, decimal=3)\n",
    "    # why is h so different ?\n",
    "    assert_almost_equal(np.linalg.norm(rbm.h_bias,ord=2),0.0012, decimal=2)\n",
    "    return rbm\n",
    "    \n",
    "rbm = test_fit_xdigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the varation in the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:13:49.051135",
     "start_time": "2016-10-05T23:03:41.709684"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_s, vb_s, hb_s = [], [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    rbm = test_fit_xdigits();\n",
    "    w_s.append(np.linalg.norm(rbm.W, ord=2))\n",
    "    vb_s.append(np.linalg.norm(rbm.v_bias, ord=2))\n",
    "    hb_s.append(np.linalg.norm(rbm.h_bias, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:13:49.335641",
     "start_time": "2016-10-05T23:13:49.053175"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean |W|  0.0204256208883\n",
      "std |W|  0.000352104017175 1.72383507508  %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8qHHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c31b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(w_s,50);\n",
    "plt.title(\"distribution of |W|\")\n",
    "print \"mean |W| \", np.mean(np.array(w_s))\n",
    "print \"std |W| \", np.std(np.array(w_s)), 100.0*np.std(np.array(w_s))/np.mean(np.array(w_s)), \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:31:57.864957",
     "start_time": "2016-10-05T23:31:57.550861"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11bc65cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEXCAYAAACpuuMDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bab5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vb_s,50);\n",
    "plt.title(\"distribution of |v_bias|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:32:04.087073",
     "start_time": "2016-10-05T23:32:03.752318"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11be57e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEXCAYAAABoPamvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHTJJREFUeJzt3XucXGWd5/HPNxfCHQKabq4JoEBmvAAil/FCKXJRVsLK\nGNCRSXDdnVVndGVnhoRdJy0zy4C7vgTlNbu6KkRXLvHCVZTAJDUzjIZrgAAhcjHh2o1cRTNEIL/9\n4zydrnRXdapzTnXVqf6+X69+5dQ5z3meX1fq9K/O85zzHEUEZmZmeUxqdwBmZlZ+TiZmZpabk4mZ\nmeXmZGJmZrk5mZiZWW5OJmZmlpuTiZmZ5eZkYk2TdImkc9PyuyWtLrDuGySdkZbnSfqXAuv+uKSf\nFVXfGNr9I0m/lPQbSSfX2b5I0t/UvP6VpPePsY1jJD0+yvb/Lem/jS3yuvVszFuHdbcp7Q7Ayiki\nbgFmb6mcpEXAARHxp1uo70PDV21NXJJmAr8CpkTExlT3ZcBlW1NfTucCX4uIi1vcTsP3KiI+3eo2\nzMBnJtYBJKnI6sj+8BVZ59aaCTzQ7iDMxoOTiTUk6VBJd0p6SdIVwLY12zbrXpF0tqQnUpfOaknv\nk3QCcA5wmqSXJa1MZZdL+jtJt0j6HbBfWvfJmuYnSfq6pBclPVDb/TO8Oyh1F303vfyn9O+LKZYj\nh3ebpe6n2yS9IOlWSUfXbFsu6dwU228k/UzSbqO8R/9R0kOSnpV0taTetP5hYD/g+lTP1Cbf9kMl\n3ZNiu1zSNk3sI0kLJf1a0qOSPl6zobZrcldJ10l6RtJzaXmvmrLzJT2S4n1E0seajNnMycTqS3/8\nrgIWA7sBPwBOHVYsUtkDgc8C74iInYETgLURcSNwHnBlROwUEYfW7PsJ4FPATsBjdUI4EngI2B3o\nA34sadcmQn9v+nfniNg5Im4dFut04HrgwlT3V4GfpPWDPgbMA94ITAP+sl5DKaGdB/wxsEf6Pa4E\niIg3AY8DJ6U4Xm0idoCPAseTJaK3A/Ob2KeX7P9oz1T+m5LeXKfcJOA7wD7AvsB64OL0u2wPXASc\nkP4P/wi4u8mYzZxMrKGjyMYdvhYRr0fEj4DbG5R9HdgGeIukKRHxWET8agv1XxoRD0bExoh4rc72\ngZq2lwBrgJPGEH+jbq6TgF9GxGWp7SuAB4EP15S5JCIeiYgNwBLgkAZ1fRz4dkTck5LFQuBoSfs2\nEUcjF0XEQES8CFw3Stu1AvhiRLwaEf8M/ASYO6JQxPMRcVVEbIiI3wF/z1Dyhez/8a2Stk0xFHaB\nhXU/JxNrZE/gyWHr1tUrGBGPAP+F7AxiQNJlg909o2h4BVJSr+09t7BPM/Zk5O+xDtir5nV/zfJ6\nYMdm6kp/oJ8bVtdYDTTZdq0XIuKVmtd13ytJ20n6hqS1kl4k6xLcVZIiYj1wGvBp4OnUBXbQ1v8a\nNtE4mVgjTzPyj+K+9QoCRMQVEfEeskFngAsGNzXaZQvt12v7qbT8O2D7mm21iWtL9T4FzKpT9/Dk\n1YynGPp9kbQDWdfZE1tRVx7TJW1X87r2var1l8CbgXdGxK4MnZUIICJuiojjyd7PNcD/bV3I1m2c\nTKyRXwCvSfoLSVMkfQQ4ol5BSQemAfdtgN8D/wYM3pcwAMzaiiu2emra/ihwMHBD2nY3cHradjjZ\nmMWgX6e2D2hQ7w3AmyWdLmmypNPILnG+bozxAVwOnCnpbZKmkY2frIiILZ11FU3AlyRNlfQesq68\nJXXK7Uj2f/ObdFFB36YKpBmSTk5jJ68CvyXr9jJrSqHJRNIXJN0n6V5J35e0jaTpkpZKWiPpRkm7\nFNmmtUYaA/gIcCZZ181HgR81KD4NOJ/sD/lTZAPXC9O2H5D9sXtO0h2D1ddrctjrFWTfop8F/hY4\nNSJeSNu+CLwJeB5YBHy/Ju5/A/4H8K+Snpe0WQKMiOeBf0f2Lf3Z9O9JNXU3fT9FRPxjiuXHZGc2\n+wGnj/I7bbHKMZYf9DTwAtl7/z3gzyLioTrlLiQ7o3sW+DlDyRmyvwVnkf0ez5KdtRR1j4pNACrq\nSYuS9gRuAQ6OiN9LupLsw/oHwHMR8WVJZwPTI2JBIY2alZiyGzojIs5tdyxbImljRLgnwxoq+sMx\nGdhB0hRgO7JvOXPILi8l/XtKwW2aWev5DngbVWHJJCKeAr5Cdq39k8BLEXEz0BMRA6lMPzCjqDbN\nSm45UN1SoXRD4svpZsLan5+0PsRNvjSObVkJFdnNtStZn/pHgZfI+sp/BHw9InarKfdcROxeSKNm\nZtYRipzo8QPAo2mAE0lXkd1FOyCpJyIG0r0Hz9TbWZJPo83MtkJEtH0uuiLHTB4DjpK0bboM9Fiy\nSe6uZWhKiHnANY0qiIjS/ixatKjtMTj+9scxEeMvc+zdEH+nKOzMJCJuk/RDYCXZdeorgW+Szb20\nRNkkfuuoM82DmZmVW6HPM4mILzFyoO55si4wMzPrUr5uvCCVSqXdIeTi+NurzPGXOXYof/ydorCr\nufLK5prrjFjMzMpCEtFlA/BmZjZBOZmYmVluTiZmZpabk4mZmeXmZGJmZrk5mZiZWW5OJmZmlpuT\niZmZ5eZkYmZmuTmZmJlZbk4mZmaWm5OJmZnl5mRiZma5OZmYmVluTiZmZpabk4mZmeXmZGITQm/v\nLCRt9tPbO6vdYZl1jcKetCjpQOBKIAAB+wNfBL6X1s8E1gJzI+KlOvv7SYvWMpLIPpqbrcWfOSu7\nTnnSYkse2ytpEvAEcCTw58BzEfFlSWcD0yNiQZ19nEysZZxMrFt1SjJpVTfXB4BHIuJxYA6wOK1f\nDJzSojbNzKxNWpVMTgMuS8s9ETEAEBH9wIwWtWlmZm1SeDKRNBU4GfhBWjW8H8H9CmZmXWZKC+r8\nIHBnRDybXg9I6omIAUm9wDONduzr69u0XKlUqFQqLQjPzKy8qtUq1Wq13WGMUPgAvKTLgZ9FxOL0\n+gLg+Yi4wAPw1i4egLdu1SkD8IUmE0nbA+uA/SPi5bRuN2AJsE/aNjciXqyzr5OJtYyTiXWrrkwm\neTiZWCs5mVi36pRk4jvgzcwsNycTMzPLzcnEzMxyczIxM7PcnEzMzCw3JxMzM8vNycTMzHJzMjEz\ns9ycTMzMLDcnEzMzy83JxMzMcnMyMTOz3JxMrOP19s5C0oif3t5Z7Q7NzBLPGmwdr/6MvzCWWX89\na7B1K88abGZmXcPJxEpsmru+zDqEu7ms443WzdVs15W7uaxbuZvLzMy6hpOJmZnlVmgykbSLpB9I\nWi3pfklHSpouaamkNZJulLRLkW2amVn7FX1mchFwQ0TMBt4OPAgsAG6OiIOAZcDCgts0M7M2K2wA\nXtLOwMqIOGDY+geBYyJiQFIvUI2Ig+vs7wF4q8sD8GaNdeMA/H7As5IukXSXpG9K2h7oiYgBgIjo\nB2YU2KaZmXWAKQXXdRjw2Yi4Q9JXybq4hn/1a/hVsK+vb9NypVKhUqkUGJ51kt7eWQwMrBuxvqdn\nJv39a8c/ILOSqFarVKvVdocxQpHdXD3ALyJi//T63WTJ5ACgUtPNtTyNqQzf391cE8hYpkhxN5dZ\nY13XzZW6sh6XdGBadSxwP3AtMD+tmwdcU1SbZmbWGQq9A17S24FvAVOBR4EzgcnAEmAfYB0wNyJe\nrLOvz0wmEJ+ZmBWjU85MPJ2KtYWTiVkxOiWZ+A54MzPLzcnEJrCRsw575mGzreNuLmuLTunmyvvQ\nLbN2czeXmZl1jSJvWjQrwLR0FmFmZeJkYh1mA/W7o8ysk7mby8zMcnMyMTOz3JxMzMwsNycTa7ne\n3lkj7uUws+7i+0ys5cZ6j0fesr7PxCYS32diZmZdw8nEzMxyczIxM7PcnEzMzCw3JxMzM8vNycTM\nzHJzMjEzs9ycTMzMLLdCZw2WtBZ4CdgIvBoRR0iaDlwJzATWAnMj4qUi2zUzs/Yq+sxkI1CJiEMj\n4oi0bgFwc0QcBCwDFhbcppmZtVnRyUR16pwDLE7Li4FTCm7TOkS9Obg8D5fZxFDo3FySHgVeBF4H\nvhER35L0QkRMrynzfETsVmdfz81VcmN9Vrvn5jLLr1Pm5ir6SYvvioinJb0RWCppDSOP1oZHaV9f\n36blSqVCpVIpODwzs3KrVqtUq9V2hzFCy2YNlrQI+C3wKbJxlAFJvcDyiJhdp7zPTErOZyZm469T\nzkwKGzORtL2kHdPyDsDxwCrgWmB+KjYPuKaoNs3MrDMU2c3VA1wlKVK934+IpZLuAJZI+iSwDphb\nYJtmLTBtxIUDPT0z6e9f255wzErAD8eywnRTN1ez9Zq1W9d1c5mZ2cTlZGJmZrk5mZiZWW5OJmZm\nlpuTiZmZ5eZkYmZmuTmZmJlZbk4mZmaWm5OJmZnl5mRiZma5OZmYmVluTiZmZpZb0Q/HMmuzkTP+\nmlnrOZlYl9lA45mAzaxV3M1lZma5OZmYNSXrPhv+09s7q92BmXUEPxzLCtMpD8ca77L+3Fo7+eFY\nZmbWNZxMzMwst8KTiaRJku6SdG16PV3SUklrJN0oaZei2zQzs/ZqxZnJ54EHal4vAG6OiIOAZcDC\nFrRpZmZtVGgykbQ38CHgWzWr5wCL0/Ji4JQi2zQzs/Yr+szkq8BfsfllLz0RMQAQEf3AjILbNDOz\nNivsDnhJJwEDEXG3pMooRRteR9nX17dpuVKpUKmMVo2Z2cRTrVapVqvtDmOEwu4zkXQe8AngNWA7\nYCfgKuBwoBIRA5J6geURMbvO/r7PpOR8n4nZ+Ou6+0wi4pyI2Dci9gdOB5ZFxBnAdcD8VGwecE1R\nbZqZWWcYj/tMzgeOk7QGODa9tpLr7Z01YmoRM5u4PJ2KbZX6XVqd3R3lbi7rRl3XzWVmZhOXk4mZ\nmeXmZGJmZrk5mZiZWW5OJma5jHxolh+YZRORr+ayreKruUYv68+yjRdfzWVmZl3DycTMzHJzMjEz\ns9ycTMzMLDcnEzMzy83JxMzMcnMyMTOz3JxMzMwsNycTMzPLzcnEzMxyczIxM7PcnEzMzCw3JxMb\nVb1nvft572Y2XGHJRNI0SbdKWilplaRFaf10SUslrZF0o6RdimrTWm9gYB3ZrLjDf8zMhhSWTCJi\nA/C+iDgUOAT4oKQjgAXAzRFxELAMWFhUm2adaeQzTiQxefIOfvaJda1Cu7kiYn1anAZMIfsKOwdY\nnNYvBk4psk2zzrOBemdzGzeuH7EuO/MzK79Ck4mkSZJWAv3ATRFxO9ATEQMAEdEPzCiyTTMza78p\nRVYWERuBQyXtDFwl6Q8Z2cHesMO9r69v03KlUqFSqRQZnplZ6VWrVarVarvDGKFlj+2V9EVgPfAp\noBIRA5J6geURMbtOeT+2twPVfzwvdPIjc8tW1p97y6PrHtsr6Q2DV2pJ2g44DlgNXAvMT8XmAdcU\n1aaZmXWGIru59gAWS5pElqSujIgbJK0Alkj6JLAOmFtgm2Zm1gFa1s01Vu7m6kzu5mp9WX/uLY+u\n6+YyM7OJy8nEzMxyczIxM7PcnEzMzCw3JxMzM8vNycTMzHJzMjEzs9ycTMzMLDcnEzMzy83JxDap\n94heM7NmeDoV26T+1CmdMeVIN5f1597y8HQqZmbWNZxMzMwsNycTMzPLzcnEzMxyczIxM7PcnEzM\nzCw3JxMzM8vNycTMzHIrLJlI2lvSMkn3S1ol6XNp/XRJSyWtkXSjpF2KatPMzDpDkWcmrwFnRcQf\nAkcDn5V0MLAAuDkiDgKWAQsLbNO2Qr1pUzx1SrtMq/t/0ds7q92BmY1Jy6ZTkXQ1cHH6OSYiBiT1\nAtWIOLhOeU+nMk7qT5sCnTzlyEQs6+PBmtHV06lImgUcAqwAeiJiACAi+oEZrWjTzMzaZ0rRFUra\nEfgh8PmI+K2k4V+vGn7d6uvr27RcqVSoVCpFh2dmVmrVapVqtdruMEYotJtL0hTgeuCnEXFRWrca\nqNR0cy2PiNl19nU31zhxN1c5yvp4sGZ0azfXd4AHBhNJci0wPy3PA64puE0zM2uzws5MJL0L+Gdg\nFdlXrQDOAW4DlgD7AOuAuRHxYp39fWYyTnxmUoay2wIbNlvT0zOT/v61dcraRNYpZyZ+ONYE5GRS\n3rI+Rmy4TkkmvgPezMxyczLpEo1uRPTNb93ENzha53I3V5cYretq+Pvqbq7uK+tjZ+LqlG6uwu8z\nsU4zzVOlmFnLOZl0vQ3U/4ZrZlYcj5mYmVluTiZmZpabk4mZmeXmZGJmZrk5mZiZWW5OJmZmlpuT\niZmZ5eZkYmZmuTmZmJlZbk4mZmaWm5NJCdWbIdjMrJ08a3AJ1Z/1t7NntXXZ1pb1sTNxdcqswT4z\nMTOz3JxMzMwst0KTiaRvSxqQdG/NuumSlkpaI+lGSbsU2aaZmbVf0WcmlwAnDFu3ALg5Ig4ClgEL\nC27TzMzarNBkEhG3AC8MWz0HWJyWFwOnFNmmmZm133iMmcyIiAGAiOgHZoxDm2ZmNo7a8djehtcw\n9vX1bVquVCpUKpVxCMes7KaNuNeop2cm/f1r2xOOtVS1WqVarbY7jBEKv89E0kzguoh4W3q9GqhE\nxICkXmB5RMyus5/vM2mS7zNx2WbK+niaGLr5PhOln0HXAvPT8jzgmha0aWZmbVT0pcGXAT8HDpT0\nmKQzgfOB4yStAY5Nr82spaaNmHKnt3dW03vXm7JnrHXYxOLpVErI3Vwuu3Vlm+/6qv8ZG1sdNj66\nuZvLzMwmGCcTMzPLzcnEzMxyczIxM7PcnEzMzCw3JxMzM8utHdOpmFlbjJx2BWDSpO3ZuHF9G+Kx\nbuJkYjZhbKDevSMbNza6T8Wsee7mMjOz3JxMzMwsNycTMzPLzcnEzMxyczIxM7PcnEzMzCw3JxMz\ny6XRs08mT97Bz0OZQPw8kxLy80xcduvKFtPW8ON0tGef+HHCrefnmZiZWddwMukQ9boK6nUT1JsO\nw6w8Rj5OuFH3lx8dXC7j1s0l6UTgQrIE9u2IuGDY9gndzeWuK5dtfdnO6OYqot6J/LdiuE7p5hqX\nubkkTQIuBo4FngJul3RNRDw4Hu2P5rHHHmPx4sUj1s+aNYszzjij6Xqq1SqVSqXAyMZbtd0B5FQF\nKm2OIY8q5Y6/vMp/7HaG8Zro8QjgoYhYByDpCmAO0PZkcumll7Jo0U1sfiAH0rymk0lv7ywGBtaN\nWF9vNtbOnaG12u4AcqpS7j/GVcoRf/2Zh9ut0THY0zOT/v61o+47WjIZy7HdTFvdbLySyV7A4zWv\nnyBLMB3ifcC5Na83Auc1vXf2YVsE9G22vt5srPVnaAXP0mrlUG/m4fZ/drNjcORxNTCQL7ZG9dY7\njvO2VXYTfgr6qVOnMm3a95g2bWXN2mD9+qlti8nMrGzGZQBe0lFAX0ScmF4vAKJ2EF6SR9TMzLZC\nJwzAj1cymQysIRuAfxq4DfhYRKxueeNmZtZy49LNFRGvS/pzYClDlwY7kZiZdYmOmU7FzMxKLCK2\n+AOcSHYZ7y+BsxuU+RrwEHA3cMiW9gWmk52prAFuBHap2bYw1bUaOL5m/WHAvamuC2vWbwNckfb5\nBbBvzbYLgLVkl6G80KHxvwe4E3gV+MiwuP4X8Pv0c2VZYgfeDvwc+BXwCtn9RaV679P2f5+2lfGz\nsw9wB9lnfwPw9yWLvwzH7heA+1PbNwH71GybR3bl6gay7v3SxM/Q8bsqbZtbL/bNfo8tFsi6pR4G\nZgJTU8UHDyvzQeAnaflIYMWW9k0flL9Oy2cD56flPwBWknXBzUr7D55B3Qq8My3fAJyQlj8N/ENa\nPg24Ii0fDfxLTQy/SG90p8W/L/AW4FI2/4O8G9lB9hbgDWQfysNLEvub0s/DwOFkyeTesrz3NTG8\nCFxFdtNtJ372R4t/OfBkimFn4J6yxE95jt1jgG3T8n9m6G/PdOAR4FHgrWm5Ez//jeJ/M3BAWt6D\n7PjdebRc0czcXJtuOIyIV8nOAOYMKzMH+C5ARNwK7CKpZwv7zgEGbz1fDJySlk9Ov9BrEbGW7AN0\nhKReYKeIuD2V+27NPrV1/RB4f1oOYHey/8hfA5OBazot/oh4LCLuY+QF7Z8Bno6I+yLiWbIPyhfK\nEHtEPEyWAB+KiDuAZ4CfUZ73HuAMYD1ZMtlIB372G8UvaTawK3BviuE3wOVliZ/yHLv/FBGvpPUr\nyO6pAziBLAGsiYhVZGcS95Ul/oh4KCIeSctPkx2/b2QUzSSTejcc7tVkmdH27YmIgRRsPzCjQV1P\n1tT1RIO6Nu0TEa8DL0naLSJWkJ3WVVI9N5L9B3da/I28Ceivef04WddFGWLfVJ+kI8i+Xd1bZ7+O\njF/Zbd5/BSxj6K68TvzsN3Ig8BpwkKQ7JV1QpvhLeuz+B+CnNXW9UlPfk2QJsizxbzJ4/A4ml0Za\nNWvw1lzzXO+bYa72JR1A9qb9v/TvsWSnb03tP0ZFxp9Hp8W+Hdk3oflNlu+U+D9D1pc/OGdGs3F1\nSvxTgNlk3zbfCRwAvLeJ/Toi/rIdu5I+AbwD+J95q9qKfVoWv6Q9aPL4bSaZPEnWrzlo77RueJl9\n6pQZbd/+dDpHOg17pom66q3fbJ90T8vOEfE82eDpncBeEbGeLOse3oHxN/Iw0Fvzeh82/+bRybFD\nNmh6MrAwnWJ34menkaOB44FPkF0EcQbZN7eyxP8E2QDtbhGxEbiabGyiLPGX5tiV9AGyge8Ppy6p\nwbq2q4lhb7JEUZb4kbQTcD1Dx+/oRhtQSYMvkxkaCNqG7FRz9rAyH2JoEOkohgaRGu5LNoh0djQe\nRNoG2I/NB5FWkPUlimwQ6cS0/jMMDcCfztAg0lyyvsqHyb6Z/SPZgFhHxV8TxyXAqTWvd2doAH4G\n2QD8ESWJfWp6v39dL4ZOj39YDGcxNABfivjJviiuJPu8zyQb4H68RPGX4tgFDh2McVhc9QbgV5Uo\n/sHj93NbyhGb9mmqUHaJ2hqyAZ0Fad2fAf+ppszFKah7gMNG2zet3w24OW1bCuxas21hqmv45W3v\nSP8hDwEX1ayfBixJ61cAs2oOqP8DrGPo8sJOjP9wsgP9ZbI/vqtqtn2FYZcGlyF24E/Se/4wWd/x\nK6RLEssQ/7AYnia7qqtsn51jyf6YvQK8BJxTlvgpz7F7U/p83EX2h/zqmm3zyc4Afk829lma+Bk6\nfgfX3wW8bbQ84ZsWzcwsNz+218zMcnMyMTOz3JxMzMwsNycTMzPLzcnEzCYkSX8s6T5Jr0s6rEGZ\nvSUtk3S/pFWSPjds+19IWp22nT9s276SXpZ0VgGxnizpHkkrJd0m6V156yzahH9sr5l1P0nHAPMj\n4sya1avIbo78xii7vgacFRF3S9oRuFPS0oh4UFIF+DDw1oh4TdIbhu37FbJ7Oopwc0Rcm36Xt5Ld\nCjG7oLoL4TMTM5soNrsPIiLWRMRDjDKFSUT0R8Tdafm3ZPdvDM5r9WmyGwZfS9ufHdxP0hyye3zu\nr61P0nGSfi7pDklXStq+qcCzWQAG7Ug28WhHcTIxs4ki13PSJc0CDiGbzh2yyTTfK2mFpOWSDk/l\ndgD+GvhSbZuSdgf+O3BsRBxONl3Mfx1D+6dIWg1cB3wyz+/SCu7mMrOuJWkF2fQiOwHTJd2VNp0d\nETeNoZ4dyR5v8fl0hgLZ38/pEXGUpHeSdT3tD/QBX42I9dnk05scRTblyb+mWamnkj2ACknnkXWZ\nDZ49KS1fHRF/AxARVwNXS3o38HfAcU2/EePAycTMulZEHAWbxkzmRcSYv9FLmkKWSL4XEdfUbHoc\n+HFq5/Y0kL872UOuTpX0ZbI5ul6X9ArwGLA0Iv6kTpznAOc0+TvdImn/9JiN58f6+7SKu7nMzEbv\nAvsO8EBEXDRs/dWkB/FJOhDYJiKei4j3RsT+EbE/cCFwXkT8A9m8ge9K0+sjaXtJzUyrPzgl/+Dy\nYamtjkkk4GRiZhNUGoN4nKz76XpJP03r95B0fVp+F9mkh+9Pl+XeJenEVMUlwP6SVgGXAX86Wntp\ngH4+cLmke8i6uA5qMtxT02XMdwFfJ5tVuaN4okczM8vNZyZmZpabk4mZmeXmZGJmZrk5mZiZWW5O\nJmZmlpuTiZmZ5eZkYmZmuTmZmJlZbv8fIBF/b+2UjyEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bd14a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(hb_s,50);\n",
    "plt.title(\"distribution of |h_bias|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test sample hiddens\n",
    "\n",
    "IDK why there is a divide-by-zero error\n",
    "\n",
    "Perhaps the exitp() needs some regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:38:44.228924",
     "start_time": "2016-10-05T23:38:44.202709"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:143: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    }
   ],
   "source": [
    "def test_sample_hiddens():\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = Xdigits[:100]\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    h = rbm1._mean_hiddens(X[0])\n",
    "    hs = np.mean([rbm1._sample_hiddens(X[0]) for i in range(100)], 0)\n",
    "\n",
    "    assert_almost_equal(h, hs, decimal=1)\n",
    "\n",
    "test_sample_hiddens()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>why divide by zero error ? <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:38:46.200299",
     "start_time": "2016-10-05T23:38:45.984744"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six.moves import cStringIO as StringIO\n",
    "def test_rbm_verbose():\n",
    "    rbm = EMF_RBM(n_iter=2, verbose=10)\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "    try:\n",
    "        rbm.fit(Xdigits)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "test_rbm_verbose()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:29:02.736552",
     "start_time": "2016-10-04T01:29:02.734464"
    }
   },
   "source": [
    "### Test Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:38:46.957196",
     "start_time": "2016-10-05T23:38:46.929169"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_transform():\n",
    "    X = Xdigits[:110] # using 100 causes divide by zero error in mean_hiddens()!\n",
    "    rbm1 = EMF_RBM(n_components=16, batch_size=5,\n",
    "                        n_iter=5, random_state=42)\n",
    "    rbm1.fit(X)\n",
    "\n",
    "    Xt1 = rbm1.transform(X)\n",
    "    Xt2 = rbm1._mean_hiddens(X)\n",
    "\n",
    "    assert_array_equal(Xt1, Xt2)\n",
    "    \n",
    "test_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test means_hidden\n",
    "\n",
    "should compare to older RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:41:18.960577",
     "start_time": "2016-10-05T23:41:18.948403"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.15573045e-06]\n",
      " [ -1.97855285e-05]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "X = np.array([[0.], [1.]])\n",
    "rbm = EMF_RBM(n_components=2, batch_size=2,\n",
    "                    n_iter=42, random_state=rng)\n",
    "# you need that much iters\n",
    "rbm.fit(X)\n",
    "#assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "#assert_almost_equal(rbm1.gibbs(X), X)\n",
    "print rbm.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:42:12.136222",
     "start_time": "2016-10-05T23:42:12.133731"
    }
   },
   "source": [
    "### <font color='red'>why is this off ? </font>\n",
    "\n",
    "Could this be due to the regularization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:48:17.628706",
     "start_time": "2016-10-05T23:48:17.608765"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.5       ]\n",
      " [ 0.50012317  0.49996447]]\n",
      "[[ 0.00049268]\n",
      " [-0.00014211]]\n"
     ]
    }
   ],
   "source": [
    "def test_mean_hiddens():\n",
    "    # Im not entirely sure why this happens, but the hidden units all go to 1/2\n",
    "    # and the h array is (2,2)\n",
    "    # h never changes ... WTf ?!\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=41, random_state=rng, \n",
    "                        decay = 0.0, weight_decay=None, momentum=0)\n",
    "    rbm.fit(X)\n",
    "    h = rbm._mean_hiddens(X)\n",
    "    assert_true(h.shape==(2,2))\n",
    "    assert_almost_equal(np.linalg.norm(h,ord=2), 1.0, decimal=4)\n",
    "    assert_almost_equal(h[0,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[0,1], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,0], 0.5, decimal=3)\n",
    "    assert_almost_equal(h[1,1], 0.5, decimal=3)\n",
    "\n",
    "    print h\n",
    "    print rbm.W\n",
    "    \n",
    "test_mean_hiddens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test equlibrate\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:43:24.735103",
     "start_time": "2016-10-05T23:43:24.720797"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.15573045e-06]\n",
      " [ -1.97855285e-05]]\n"
     ]
    }
   ],
   "source": [
    "def test_fit_equilibrate():\n",
    "    # Equlibrate on the RBM hidden layer should be able to recreate [[0], [1]]\n",
    "    # from the same input\n",
    "    rng = np.random.RandomState(42)\n",
    "    X = np.array([[0.], [1.]])\n",
    "    rbm1 = EMF_RBM(n_components=2, batch_size=2,\n",
    "                        n_iter=42, random_state=rng)\n",
    "    # you need that much iters\n",
    "    rbm1.fit(X)\n",
    "    #assert_almost_equal(rbm1.W, np.array([[0.02649814], [0.02009084]]), decimal=4)\n",
    "    #assert_almost_equal(rbm1.gibbs(X), X)\n",
    "    return rbm1, X\n",
    "\n",
    "rbm, X = test_fit_equilibrate()\n",
    "print rbm.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using sparse CSR matrix\n",
    "\n",
    "### <font color='red'>must implement sparse matrices: not so simple <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T23:39:52.132834",
     "start_time": "2016-10-05T23:39:52.067012"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:143: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse matrix is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8b4873d22414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mEMF_RBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# no exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_small_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-8b4873d22414>\u001b[0m in \u001b[0;36mtest_small_sparse\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# EMF_RBM should work on small sparse matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdigits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mEMF_RBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# no exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_small_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5af9e90df4a0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;31m#print \"iter \", iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_slices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;31m#print \"batches done\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5af9e90df4a0>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, v_pos)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m# get_negative_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mv_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequilibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneq_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# basic gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5af9e90df4a0>\u001b[0m in \u001b[0;36mequilibrate\u001b[0;34m(self, v0, h0, iters)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mmh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmh_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5af9e90df4a0>\u001b[0m in \u001b[0;36mmv_update\u001b[0;34m(self, v, h)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mh_fluc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mh_fluc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;31m#a += safe_sparse_dot(h_fluc,self.W2)*(0.5-v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Now we would add this scalar to every element.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 raise NotImplementedError('adding a nonzero scalar to a '\n\u001b[0m\u001b[1;32m    379\u001b[0m                                           'sparse matrix is not supported')\n\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse matrix is not supported"
     ]
    }
   ],
   "source": [
    "def test_small_sparse():\n",
    "    # EMF_RBM should work on small sparse matrices.\n",
    "    X = csr_matrix(Xdigits[:4])\n",
    "    EMF_RBM().fit(X)       # no exception\n",
    "\n",
    "test_small_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more tests TODO\n",
    "\n",
    "#### in sklearn\n",
    "- test_small_sparse():\n",
    "- test_small_sparse_partial_fit():\n",
    "- test_sample_hiddens():\n",
    "- test_equilibrate():\n",
    "- test_equilibrate_sparse():\n",
    "- test_equilibrate_smoke():\n",
    "- test_score_samples():\n",
    "- test_sparse_and_verbose():\n",
    "\n",
    "\n",
    "#### additional\n",
    "\n",
    "- test_entropy\n",
    "- ? test_free_energy (not necessary since we have score samples) \n",
    "- ? test_energy (not with entropy, but we could just subtract)\n",
    "\n",
    "#### system tests\n",
    "\n",
    "- check that code works with 2 or more iterations\n",
    "- check  mnist, with accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-04T01:06:25.316119",
     "start_time": "2016-10-04T01:06:25.298496"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try classifier\n",
    "\n",
    "#### should we be using the EMF estimator?\n",
    "\n",
    "what are the correlations...do they drop to 0 as we converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.498831",
     "start_time": "2016-10-05T21:22:24.636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499224",
     "start_time": "2016-10-05T21:22:24.819Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = sig_means(X, rbm.h_bias , rbm.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.499573",
     "start_time": "2016-10-05T21:22:24.986Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print p.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500034",
     "start_time": "2016-10-05T21:22:25.162Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(p, Y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.500518",
     "start_time": "2016-10-05T21:22:25.332Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in [5000]:\n",
    "    lr  = linear_model.LogisticRegression()\n",
    "    lr.C = c\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_test_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "    print c, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T14:24:25.501155",
     "start_time": "2016-10-05T21:22:25.515Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note bad, but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
